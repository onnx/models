{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook for squeezenet 1.0 and 1.1 on Imagenet dataset\n",
    "Use this notebook to train a squeezenet model from scratch. **Make sure to have the imagenet dataset prepared** according to the guidelines in the dataset section in [squeezenet readme](README.md) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mxnet-cu90mkl #tested on this version GPU, can use other versions\n",
    "!pip install gluoncv\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "Verify that all dependencies are installed using the cell below. Continue if no errors encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import time, logging\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data import imagenet\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "\n",
    "import os\n",
    "from mxnet.context import cpu\n",
    "from mxnet.gluon.block import HybridBlock\n",
    "from mxnet.gluon.contrib.nn import HybridConcurrent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify model, hyperparameters and save locations\n",
    "The values shown below were used to train the model in the model zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The training was done on P3.8xlarge AWS ec2 instance. Configure hyperparameters like num_gpus, batch_size, context \n",
    "    and num_workers based on hardware you are using\n",
    "'''\n",
    "\n",
    "# specify model - squeezenet1.0 or squeezenet1.1\n",
    "model_name = 'squeezenet1.0'\n",
    "\n",
    "# training and validation pictures to use\n",
    "data_dir = '../imagenet/img_dataset'\n",
    "\n",
    "# training batch size per device (CPU/GPU)\n",
    "batch_size = 128\n",
    "\n",
    "# number of GPUs to use\n",
    "num_gpus = 4\n",
    "\n",
    "# number of pre-processing workers\n",
    "num_workers = 32\n",
    "\n",
    "# number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# momentum value for optimizer\n",
    "momentum = 0.9\n",
    "\n",
    "# weight decay rate\n",
    "wd = 0.0002\n",
    "\n",
    "# decay rate of learning rate\n",
    "lr_decay = 0.1\n",
    "\n",
    "# interval for periodic learning rate decays\n",
    "lr_decay_period = 0\n",
    "\n",
    "# epoches at which learning rate decays\n",
    "lr_decay_epoch = '60,90'\n",
    "\n",
    "# mode in which to train the model. options are symbolic, imperative, hybrid\n",
    "mode = 'hybrid'\n",
    "\n",
    "# use label smoothing or not in training\n",
    "label_smoothing = False\n",
    "\n",
    "# Number of batches to wait before logging\n",
    "log_interval = 100\n",
    "\n",
    "# frequency of model saving\n",
    "save_frequency = 10\n",
    "\n",
    "# directory of saved models\n",
    "save_dir = 'params'\n",
    "\n",
    "#directory of training logs\n",
    "logging_dir = 'logs'\n",
    "\n",
    "# the path to save the history plot\n",
    "save_plot_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition in Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SqueezeNet, implemented in Gluon.\"\"\"\n",
    "__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
    "\n",
    "# Helpers\n",
    "def _make_fire(squeeze_channels, expand1x1_channels, expand3x3_channels):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    out.add(_make_fire_conv(squeeze_channels, 1))\n",
    "\n",
    "    paths = HybridConcurrent(axis=1, prefix='')\n",
    "    paths.add(_make_fire_conv(expand1x1_channels, 1))\n",
    "    paths.add(_make_fire_conv(expand3x3_channels, 3, 1))\n",
    "    out.add(paths)\n",
    "\n",
    "    return out\n",
    "\n",
    "def _make_fire_conv(channels, kernel_size, padding=0):\n",
    "    out = nn.HybridSequential(prefix='')\n",
    "    out.add(nn.Conv2D(channels, kernel_size, padding=padding))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out\n",
    "\n",
    "# Net\n",
    "class SqueezeNet(HybridBlock):\n",
    "    r\"\"\"SqueezeNet model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
    "    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "    SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    version : str\n",
    "        Version of squeezenet. Options are '1.0', '1.1'.\n",
    "    classes : int, default 1000\n",
    "        Number of classification classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, version, classes=1000, **kwargs):\n",
    "        super(SqueezeNet, self).__init__(**kwargs)\n",
    "        assert version in ['1.0', '1.1'], (\"Unsupported SqueezeNet version {version}:\"\n",
    "                                           \"1.0 or 1.1 expected\".format(version=version))\n",
    "        with self.name_scope():\n",
    "            self.features = nn.HybridSequential(prefix='')\n",
    "            if version == '1.0':\n",
    "                self.features.add(nn.Conv2D(96, kernel_size=7, strides=2))\n",
    "                self.features.add(nn.Activation('relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "            else:\n",
    "                self.features.add(nn.Conv2D(64, kernel_size=3, strides=2))\n",
    "                self.features.add(nn.Activation('relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(_make_fire(16, 64, 64))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(_make_fire(32, 128, 128))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2, ceil_mode=True))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(48, 192, 192))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "                self.features.add(_make_fire(64, 256, 256))\n",
    "            self.features.add(nn.Dropout(0.5))\n",
    "\n",
    "            self.output = nn.HybridSequential(prefix='')\n",
    "            self.output.add(nn.Conv2D(classes, kernel_size=1))\n",
    "            self.output.add(nn.Activation('relu'))\n",
    "            self.output.add(nn.AvgPool2D(13))\n",
    "            self.output.add(nn.Flatten())\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.features(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Constructor\n",
    "def get_squeezenet(version, root=os.path.join('~', '.mxnet', 'models'), **kwargs):\n",
    "    r\"\"\"SqueezeNet model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
    "    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "    SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    version : str\n",
    "        Version of squeezenet. Options are '1.0', '1.1'.\n",
    "    root : str, default '~/.mxnet/models'\n",
    "        Location for keeping the model parameters.\n",
    "    \"\"\"\n",
    "    net = SqueezeNet(version, **kwargs)\n",
    "    return net\n",
    "\n",
    "def squeezenet1_0(**kwargs):\n",
    "    r\"\"\"SqueezeNet 1.0 model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n",
    "    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str, default '~/.mxnet/models'\n",
    "        Location for keeping the model parameters.\n",
    "    \"\"\"\n",
    "    return get_squeezenet('1.0', **kwargs)\n",
    "\n",
    "def squeezenet1_1(**kwargs):\n",
    "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
    "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
    "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
    "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str, default '~/.mxnet/models'\n",
    "        Location for keeping the model parameters.\n",
    "    \"\"\"\n",
    "    return get_squeezenet('1.1', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "classes = 1000\n",
    "batch_size *= max(1, num_gpus)\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "\n",
    "lr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')] + [np.inf]\n",
    "\n",
    "kwargs = {'classes': classes}\n",
    "\n",
    "optimizer = 'nag'\n",
    "optimizer_params = {'learning_rate': lr, 'wd': wd, 'momentum': momentum}\n",
    "\n",
    "if model_name == 'squeezenet1.0':\n",
    "    net = squeezenet1_0(**kwargs)\n",
    "else:\n",
    "    net = squeezenet1_1(**kwargs)\n",
    "\n",
    "acc_top1 = mx.metric.Accuracy()\n",
    "acc_top5 = mx.metric.TopKAccuracy(5)\n",
    "train_history = TrainingHistory(['training-top1-err', 'training-top5-err',\n",
    "                                 'validation-top1-err', 'validation-top5-err'])\n",
    "\n",
    "makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "# Input pre-processing for train data\n",
    "def preprocess_train_data(normalize, jitter_param, lighting_param):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomFlipLeftRight(),\n",
    "        transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n",
    "                                     saturation=jitter_param),\n",
    "        transforms.RandomLighting(lighting_param),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return transform_train\n",
    "\n",
    "# Input pre-processing for validation data\n",
    "def preprocess_test_data(normalize):\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return transform_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(label, classes, eta=0.1):\n",
    "    if isinstance(label, nd.NDArray):\n",
    "        label = [label]\n",
    "    smoothed = []\n",
    "    for l in label:\n",
    "        ind = l.astype('int')\n",
    "        res = nd.zeros((ind.shape[0], classes), ctx = l.context)\n",
    "        res += eta/classes\n",
    "        res[nd.arange(ind.shape[0], ctx = l.context), ind] = 1 - eta + eta/classes\n",
    "        smoothed.append(res)\n",
    "    return smoothed\n",
    "\n",
    "# Test function\n",
    "def test(ctx, val_data):\n",
    "    acc_top1.reset()\n",
    "    acc_top5.reset()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        acc_top1.update(label, outputs)\n",
    "        acc_top5.update(label, outputs)\n",
    "\n",
    "    _, top1 = acc_top1.get()\n",
    "    _, top5 = acc_top5.get()\n",
    "    return (1-top1, 1-top5)\n",
    "\n",
    "# Train function\n",
    "def train(epochs, ctx):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    net.initialize(mx.init.MSRAPrelu(), ctx=ctx)\n",
    "    # Prepare train and validation batches\n",
    "    transform_train = preprocess_train_data(normalize, jitter_param, lighting_param)\n",
    "    transform_test = preprocess_test_data(normalize)\n",
    "    train_data = gluon.data.DataLoader(\n",
    "        imagenet.classification.ImageNet(data_dir, train=True).transform_first(transform_train),\n",
    "        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "    val_data = gluon.data.DataLoader(\n",
    "        imagenet.classification.ImageNet(data_dir, train=False).transform_first(transform_test),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    # Define trainer\n",
    "    trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "    if label_smoothing:\n",
    "        L = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "    else:\n",
    "        L = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    lr_decay_count = 0\n",
    "\n",
    "    best_val_score = 1\n",
    "    # Main training loop\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        acc_top1.reset()\n",
    "        acc_top5.reset()\n",
    "        btic = time.time()\n",
    "        train_loss = 0\n",
    "        num_batch = len(train_data)\n",
    "\n",
    "        if lr_decay_period and epoch and epoch % lr_decay_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        elif lr_decay_period == 0 and epoch == lr_decay_epoch[lr_decay_count]:\n",
    "            trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "            lr_decay_count += 1\n",
    "\n",
    "        for i, batch in enumerate(train_data):\n",
    "            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "            if label_smoothing:\n",
    "                label_smooth = smooth(label, classes)\n",
    "            else:\n",
    "                label_smooth = label\n",
    "            with ag.record():\n",
    "                outputs = [net(X) for X in data]\n",
    "                loss = [L(yhat, y) for yhat, y in zip(outputs, label_smooth)]\n",
    "            ag.backward(loss)\n",
    "            trainer.step(batch_size)\n",
    "            # Initialize last conv layer weights with normal distribution after first forward pass\n",
    "            if i==0 and epoch==0:\n",
    "                new_classifier_w = mx.nd.random_normal(shape=(1000, 512, 1, 1), scale=0.01)\n",
    "                final_conv_layer_params = net.output[0].params\n",
    "                final_conv_layer_params.get('weight').set_data(new_classifier_w)\n",
    "            acc_top1.update(label, outputs)\n",
    "            acc_top5.update(label, outputs)\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            if log_interval and not (i+1)%log_interval:\n",
    "                _, top1 = acc_top1.get()\n",
    "                _, top5 = acc_top5.get()\n",
    "                err_top1, err_top5 = (1-top1, 1-top5)\n",
    "                logging.info('Epoch[%d] Batch [%d]\\tSpeed: %f samples/sec\\ttop1-err=%f\\ttop5-err=%f'%(\n",
    "                             epoch, i, batch_size*log_interval/(time.time()-btic), err_top1, err_top5))\n",
    "                btic = time.time()\n",
    "\n",
    "        _, top1 = acc_top1.get()\n",
    "        _, top5 = acc_top5.get()\n",
    "        err_top1, err_top5 = (1-top1, 1-top5)\n",
    "        train_loss /= num_batch * batch_size\n",
    "\n",
    "        err_top1_val, err_top5_val = test(ctx, val_data)\n",
    "        train_history.update([err_top1, err_top5, err_top1_val, err_top5_val])\n",
    "        train_history.plot(['training-top1-err', 'validation-top1-err','training-top5-err', 'validation-top5-err'],\n",
    "                           save_path='%s/%s_top_error.png'%(save_plot_dir, model_name))\n",
    "\n",
    "        logging.info('[Epoch %d] training: err-top1=%f err-top5=%f loss=%f'%(epoch, err_top1, err_top5, train_loss))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        logging.info('[Epoch %d] validation: err-top1=%f err-top5=%f'%(epoch, err_top1_val, err_top5_val))\n",
    "\n",
    "        if err_top1_val < best_val_score and epoch > 50:\n",
    "            best_val_score = err_top1_val\n",
    "            net.export('%s/%.4f-imagenet-%s-best'%(save_dir, best_val_score, model_name), epoch)\n",
    "        if save_frequency and save_dir and (epoch + 1) % save_frequency == 0:\n",
    "            net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epoch)\n",
    "\n",
    "    if save_frequency and save_dir:\n",
    "        net.export('%s/%.4f-imagenet-%s'%(save_dir, best_val_score, model_name), epochs-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n",
    "* Run the cell below to start training\n",
    "* Logs are displayed in the cell output\n",
    "* An example run of 1 epoch is shown here\n",
    "* Once training completes, the symbols and params files are saved in the root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [99]\tSpeed: 909.102113 samples/sec\ttop1-err=0.998906\ttop5-err=0.995000\n",
      "INFO:root:Epoch[0] Batch [199]\tSpeed: 1557.133090 samples/sec\ttop1-err=0.998936\ttop5-err=0.994873\n",
      "INFO:root:Epoch[0] Batch [299]\tSpeed: 1447.621584 samples/sec\ttop1-err=0.998913\ttop5-err=0.994974\n",
      "INFO:root:Epoch[0] Batch [399]\tSpeed: 1491.048439 samples/sec\ttop1-err=0.998857\ttop5-err=0.994800\n",
      "INFO:root:Epoch[0] Batch [499]\tSpeed: 1433.206399 samples/sec\ttop1-err=0.998891\ttop5-err=0.994711\n",
      "INFO:root:Epoch[0] Batch [599]\tSpeed: 1454.076697 samples/sec\ttop1-err=0.998854\ttop5-err=0.994499\n",
      "INFO:root:Epoch[0] Batch [699]\tSpeed: 1434.658017 samples/sec\ttop1-err=0.998783\ttop5-err=0.994272\n",
      "INFO:root:Epoch[0] Batch [799]\tSpeed: 1431.959446 samples/sec\ttop1-err=0.998674\ttop5-err=0.993906\n",
      "INFO:root:Epoch[0] Batch [899]\tSpeed: 1398.024324 samples/sec\ttop1-err=0.998596\ttop5-err=0.993414\n",
      "INFO:root:Epoch[0] Batch [999]\tSpeed: 1444.611202 samples/sec\ttop1-err=0.998506\ttop5-err=0.992977\n",
      "INFO:root:Epoch[0] Batch [1099]\tSpeed: 1445.752342 samples/sec\ttop1-err=0.998448\ttop5-err=0.992582\n",
      "INFO:root:Epoch[0] Batch [1199]\tSpeed: 1445.734044 samples/sec\ttop1-err=0.998394\ttop5-err=0.992248\n",
      "INFO:root:Epoch[0] Batch [1299]\tSpeed: 1468.081600 samples/sec\ttop1-err=0.998337\ttop5-err=0.991895\n",
      "INFO:root:Epoch[0] Batch [1399]\tSpeed: 1433.247023 samples/sec\ttop1-err=0.998224\ttop5-err=0.991420\n",
      "INFO:root:Epoch[0] Batch [1499]\tSpeed: 1428.776457 samples/sec\ttop1-err=0.998086\ttop5-err=0.990910\n",
      "INFO:root:Epoch[0] Batch [1599]\tSpeed: 1419.937457 samples/sec\ttop1-err=0.997927\ttop5-err=0.990352\n",
      "INFO:root:Epoch[0] Batch [1699]\tSpeed: 1417.843971 samples/sec\ttop1-err=0.997808\ttop5-err=0.989836\n",
      "INFO:root:Epoch[0] Batch [1799]\tSpeed: 1423.802762 samples/sec\ttop1-err=0.997694\ttop5-err=0.989280\n",
      "INFO:root:Epoch[0] Batch [1899]\tSpeed: 1439.332638 samples/sec\ttop1-err=0.997542\ttop5-err=0.988745\n",
      "INFO:root:Epoch[0] Batch [1999]\tSpeed: 1419.864088 samples/sec\ttop1-err=0.997407\ttop5-err=0.988147\n",
      "INFO:root:Epoch[0] Batch [2099]\tSpeed: 1411.885732 samples/sec\ttop1-err=0.997287\ttop5-err=0.987530\n",
      "INFO:root:Epoch[0] Batch [2199]\tSpeed: 1411.949980 samples/sec\ttop1-err=0.997127\ttop5-err=0.986890\n",
      "INFO:root:Epoch[0] Batch [2299]\tSpeed: 1414.518012 samples/sec\ttop1-err=0.996912\ttop5-err=0.986152\n",
      "INFO:root:Epoch[0] Batch [2399]\tSpeed: 1399.267654 samples/sec\ttop1-err=0.996715\ttop5-err=0.985349\n",
      "INFO:root:Epoch[0] Batch [2499]\tSpeed: 1546.679873 samples/sec\ttop1-err=0.996484\ttop5-err=0.984505\n",
      "INFO:root:[Epoch 0] training: err-top1=0.996479 err-top5=0.984493 loss=6.786876\n",
      "INFO:root:[Epoch 0] time cost: 960.695095\n",
      "INFO:root:[Epoch 0] validation: err-top1=0.990300 err-top5=0.960580\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    net.hybridize()\n",
    "    train(num_epochs, context)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to ONNX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
