[
    {
        "model": "BERT-Squad",
        "model_path": "text/machine_comprehension/bert-squad/model/bertsquad-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "5945dee6478abdab2d5e4ce3868b4d969741e3dad2134cc669da65a4f092755b",
            "model_bytes": 435852734,
            "tags": [
                "text",
                "machine comprehension",
                "bert-squad"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "unique_ids_raw_output___9:0",
                        "shape": [
                            "unk__492"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "segment_ids:0",
                        "shape": [
                            "unk__493",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_mask:0",
                        "shape": [
                            "unk__494",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_ids:0",
                        "shape": [
                            "unk__495",
                            256
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "unstack:1",
                        "shape": [
                            "unk__496",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unstack:0",
                        "shape": [
                            "unk__497",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unique_ids:0",
                        "shape": [
                            "unk__498"
                        ],
                        "type": "tensor(int64)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bert-squad/model/bertsquad-10.tar.gz",
            "model_with_data_sha": "0ff18af268a891e7de390c5476191084e95eafba2763a69091c83ced7030c8b2",
            "model_with_data_bytes": 403398451
        }
    },
    {
        "model": "BERT-Squad-int8",
        "model_path": "text/machine_comprehension/bert-squad/model/bertsquad-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "24325bdf732256859e07a385d2f363a6839b20b2bb4a57ac362b3982fc8ce121",
            "model_bytes": 124565601,
            "tags": [
                "text",
                "machine comprehension",
                "bert-squad"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "unique_ids_raw_output___9:0",
                        "shape": [
                            "unk__492"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "segment_ids:0",
                        "shape": [
                            "unk__493",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_mask:0",
                        "shape": [
                            "unk__494",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_ids:0",
                        "shape": [
                            "unk__495",
                            256
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "unstack:1",
                        "shape": [
                            "unk__496",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unstack:0",
                        "shape": [
                            "unk__497",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unique_ids:0",
                        "shape": [
                            "unk__498"
                        ],
                        "type": "tensor(int64)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bert-squad/model/bertsquad-12-int8.tar.gz",
            "model_with_data_sha": "6c33f44ee1949ee25936e259cb68da3e19bea7f6e7d5ea72a4d95ae300c86d87",
            "model_with_data_bytes": 106044512
        }
    },
    {
        "model": "BERT-Squad",
        "model_path": "text/machine_comprehension/bert-squad/model/bertsquad-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "5f0d96a9e6b8a4a2e59f9636f8bbad09ee8a0c58c8212027cc17c5fcc9659e55",
            "model_bytes": 435852736,
            "tags": [
                "text",
                "machine comprehension",
                "bert-squad"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "unique_ids_raw_output___9:0",
                        "shape": [
                            "unk__492"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "segment_ids:0",
                        "shape": [
                            "unk__493",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_mask:0",
                        "shape": [
                            "unk__494",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_ids:0",
                        "shape": [
                            "unk__495",
                            256
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "unstack:1",
                        "shape": [
                            "unk__496",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unstack:0",
                        "shape": [
                            "unk__497",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unique_ids:0",
                        "shape": [
                            "unk__498"
                        ],
                        "type": "tensor(int64)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bert-squad/model/bertsquad-12.tar.gz",
            "model_with_data_sha": "4cd041010ab4ad11c23c0eb7f056c4e4286a894b6d712ef09445b955763fb1b1",
            "model_with_data_bytes": 403082198
        }
    },
    {
        "model": "BERT-Squad",
        "model_path": "text/machine_comprehension/bert-squad/model/bertsquad-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "cad65b9807a5e0393e4f84331f9a0c5c844d9cc736e39781a80f9c48ca39447c",
            "model_bytes": 435882893,
            "tags": [
                "text",
                "machine comprehension",
                "bert-squad"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "unique_ids_raw_output___9:0",
                        "shape": [
                            "unk__475"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "segment_ids:0",
                        "shape": [
                            "unk__476",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_mask:0",
                        "shape": [
                            "unk__477",
                            256
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "input_ids:0",
                        "shape": [
                            "unk__478",
                            256
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "unstack:1",
                        "shape": [
                            "unk__479",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unstack:0",
                        "shape": [
                            "unk__480",
                            256
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "unique_ids:0",
                        "shape": [
                            "unk__481"
                        ],
                        "type": "tensor(int64)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bert-squad/model/bertsquad-8.tar.gz",
            "model_with_data_sha": "c8c6c7e0ab9e1333b86e8415a9d990b2570f9374f80be1c1cb72f182d266f666",
            "model_with_data_bytes": 403400046
        }
    },
    {
        "model": "BiDAF-int8",
        "model_path": "text/machine_comprehension/bidirectional_attention_flow/model/bidaf-11-int8.onnx",
        "onnx_version": "1.13.1",
        "opset_version": 11,
        "metadata": {
            "model_sha": "c2bbfd7568f4f19c8db82395c81d8d6199f3c0237f49e0f669d47c82643ef29e",
            "model_bytes": 12452924,
            "tags": [
                "text",
                "machine comprehension",
                "bidirectional attention flow"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "context_word",
                        "shape": [
                            "c",
                            1
                        ],
                        "type": "tensor(string)"
                    },
                    {
                        "name": "context_char",
                        "shape": [
                            "c",
                            1,
                            1,
                            16
                        ],
                        "type": "tensor(string)"
                    },
                    {
                        "name": "query_word",
                        "shape": [
                            "q",
                            1
                        ],
                        "type": "tensor(string)"
                    },
                    {
                        "name": "query_char",
                        "shape": [
                            "q",
                            1,
                            1,
                            16
                        ],
                        "type": "tensor(string)"
                    }
                ],
                "outputs": [
                    {
                        "name": "start_pos",
                        "shape": [
                            1
                        ],
                        "type": "tensor(int32)"
                    },
                    {
                        "name": "end_pos",
                        "shape": [
                            1
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bidirectional_attention_flow/model/bidaf-11-int8.tar.gz",
            "model_with_data_sha": "571410c31445882ea9ed7b9f48fe8c2ed6ccb72b925281a1be82a75c0c12b6ab",
            "model_with_data_bytes": 9086295
        }
    },
    {
        "model": "BiDAF",
        "model_path": "text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "dfc317b56d065a3e297240a9e9b9118ff2260790b5850f4be2bc6ea1bcc65e80",
            "model_bytes": 43522228,
            "tags": [
                "text",
                "machine comprehension",
                "bidirectional attention flow"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "context_word",
                        "shape": [
                            "c",
                            1
                        ],
                        "type": "tensor(string)"
                    },
                    {
                        "name": "context_char",
                        "shape": [
                            "c",
                            1,
                            1,
                            16
                        ],
                        "type": "tensor(string)"
                    },
                    {
                        "name": "query_word",
                        "shape": [
                            "q",
                            1
                        ],
                        "type": "tensor(string)"
                    },
                    {
                        "name": "query_char",
                        "shape": [
                            "q",
                            1,
                            1,
                            16
                        ],
                        "type": "tensor(string)"
                    }
                ],
                "outputs": [
                    {
                        "name": "start_pos",
                        "shape": [
                            1
                        ],
                        "type": "tensor(int32)"
                    },
                    {
                        "name": "end_pos",
                        "shape": [
                            1
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/bidirectional_attention_flow/model/bidaf-9.tar.gz",
            "model_with_data_sha": "c74387eec257f2cb37cefc2846e1c4078bfebf06cd6486e9dafe6c9f7cdc1ef3",
            "model_with_data_bytes": 39092248
        }
    },
    {
        "model": "GPT-2",
        "model_path": "text/machine_comprehension/gpt-2/model/gpt2-10.onnx",
        "onnx_version": "1.6",
        "opset_version": 10,
        "metadata": {
            "model_sha": "a4b41071a9fd8fd5591d74d6c7e3a850feaf161f7289148a3b845c84f75c55e0",
            "model_bytes": 548227537,
            "tags": [
                "text",
                "machine comprehension",
                "gpt-2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            "input1_dynamic_axes_1",
                            "input1_dynamic_axes_2",
                            "input1_dynamic_axes_3"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            1,
                            8,
                            768
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output2",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output3",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output4",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output5",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output6",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output7",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output8",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output9",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output10",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output11",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output12",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output13",
                        "shape": [
                            2,
                            1,
                            12,
                            8,
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/gpt-2/model/gpt2-10.tar.gz",
            "model_with_data_sha": "0c9b81350c95cf2f40e8e72f76792ef6b3c73894beee571171739079330aad53",
            "model_with_data_bytes": 463131530
        }
    },
    {
        "model": "GPT-2-LM-HEAD",
        "model_path": "text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.onnx",
        "onnx_version": "1.6",
        "opset_version": 10,
        "metadata": {
            "model_sha": "12fbb1ec2d2d70c8ebd21a290a348a4109447b98582af64c6f93b6526d5c8f35",
            "model_bytes": 664871060,
            "tags": [
                "text",
                "machine comprehension",
                "gpt-2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            "input1_dynamic_axes_1",
                            "input1_dynamic_axes_2",
                            "input1_dynamic_axes_3"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            "input1_dynamic_axes_1",
                            "input1_dynamic_axes_2",
                            "input1_dynamic_axes_3",
                            50257
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output2",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output3",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output4",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output5",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output6",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output7",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output8",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output9",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output10",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output11",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output12",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output13",
                        "shape": [
                            2,
                            "input1_dynamic_axes_2",
                            12,
                            "input1_dynamic_axes_3",
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.tar.gz",
            "model_with_data_sha": "820f53e5403bb9fa0dac10af138cedcb71f282a527f04d76e35db5b2b88871c8",
            "model_with_data_bytes": 606542743
        }
    },
    {
        "model": "RoBERTa-BASE",
        "model_path": "text/machine_comprehension/roberta/model/roberta-base-11.onnx",
        "onnx_version": "1.6",
        "opset_version": 11,
        "metadata": {
            "model_sha": "ad476a33a4b227f6e6b2e1c7192df1b61640657f26b390857ab943de66236c0b",
            "model_bytes": 498649858,
            "tags": [
                "text",
                "machine comprehension",
                "roberta"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_ids",
                        "shape": [
                            "batch_size",
                            "seq_len"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output_1",
                        "shape": [
                            "batch_size",
                            "seq_len",
                            768
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output_2",
                        "shape": [
                            "batch_size",
                            768
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/roberta/model/roberta-base-11.tar.gz",
            "model_with_data_sha": "16134da10f8fee3283d22a0d70928fe5ceea1ca2ea23507fe0b0f295108a8815",
            "model_with_data_bytes": 291239125
        }
    },
    {
        "model": "RoBERTa-SequenceClassification",
        "model_path": "text/machine_comprehension/roberta/model/roberta-sequence-classification-9.onnx",
        "onnx_version": "1.6",
        "opset_version": 9,
        "metadata": {
            "model_sha": "8b28f588889de84ef45918765b25bd0e74179b9be5d9320dafdddcb2553784cc",
            "model_bytes": 498658080,
            "tags": [
                "text",
                "machine comprehension",
                "roberta"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            "sentence_length"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/roberta/model/roberta-sequence-classification-9.tar.gz",
            "model_with_data_sha": "1fb22feddadc6a0e26f65b6022f0d77d3585d24866b88a7126ab243451032c5b",
            "model_with_data_bytes": 431071460
        }
    },
    {
        "model": "T5-decoder-with-lm-head",
        "model_path": "text/machine_comprehension/t5/model/t5-decoder-with-lm-head-12.onnx",
        "onnx_version": "1.7",
        "opset_version": 12,
        "metadata": {
            "model_sha": "235afca35d3c47a5fd7209844ac11f3157fe7263fd074197a45b7f536e40ea56",
            "model_bytes": 650564941,
            "tags": [
                "text",
                "machine comprehension",
                "t5"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_ids",
                        "shape": [
                            "batch",
                            "sequence"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "encoder_hidden_states",
                        "shape": [
                            "batch",
                            "sequence",
                            768
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "hidden_states",
                        "shape": [
                            "batch",
                            "sequence",
                            32128
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/t5/model/t5-decoder-with-lm-head-12.tar.gz",
            "model_with_data_sha": "fa6edefa5951479f7b68bea693950a4ebc188f54c7971bf73f2b036b91bc8066",
            "model_with_data_bytes": 287840759
        }
    },
    {
        "model": "T5-encoder",
        "model_path": "text/machine_comprehension/t5/model/t5-encoder-12.onnx",
        "onnx_version": "1.7",
        "opset_version": 12,
        "metadata": {
            "model_sha": "4523122d7cf0f50905694d84995633c8a0cc223da762d3eb2aaffa17251a6f60",
            "model_bytes": 438549611,
            "tags": [
                "text",
                "machine comprehension",
                "t5"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_ids",
                        "shape": [
                            "batch",
                            "sequence"
                        ],
                        "type": "tensor(int64)"
                    }
                ],
                "outputs": [
                    {
                        "name": "hidden_states",
                        "shape": [
                            "batch",
                            "sequence",
                            768
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "text/machine_comprehension/t5/model/t5-encoder-12.tar.gz",
            "model_with_data_sha": "434e2f691bd71c838bd4b68be47bdf32c899313fdebfe4b77c7bea0b2f52e831",
            "model_with_data_bytes": 194535656
        }
    },
    {
        "model": "LResNet100E-IR-int8",
        "model_path": "vision/body_analysis/arcface/model/arcfaceresnet100-11-int8.onnx",
        "onnx_version": "1.13.1",
        "opset_version": 11,
        "metadata": {
            "model_sha": "c625ca68a422418c48aa84f73341337e0a92b111f327909005d1eec07c95f936",
            "model_bytes": 65764892,
            "tags": [
                "vision",
                "body analysis",
                "arcface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            112,
                            112
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc1",
                        "shape": [
                            1,
                            512
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/arcface/model/arcfaceresnet100-11-int8.tar.gz",
            "model_with_data_sha": "d560f59c57fa4784771ba520b5b2f380097d7b2210e6c8b02ca203c2e9784f8a",
            "model_with_data_bytes": 47945269
        }
    },
    {
        "model": "LResNet100E-IR",
        "model_path": "vision/body_analysis/arcface/model/arcfaceresnet100-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "f3a6bc281e72f88862f5748b53be3d76b3b48f8f1ab1f4a537941bdc4e1b01da",
            "model_bytes": 261036388,
            "tags": [
                "vision",
                "body analysis",
                "arcface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            112,
                            112
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc1",
                        "shape": [
                            1,
                            512
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/arcface/model/arcfaceresnet100-8.tar.gz",
            "model_with_data_sha": "f7098d004468725906a805e50d772eabc95e31f57eedeaaa72aeab8225598c2a",
            "model_with_data_bytes": 237272167
        }
    },
    {
        "model": "Emotion FERPlus",
        "model_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-2.onnx",
        "onnx_version": "1.0",
        "opset_version": 2,
        "metadata": {
            "model_sha": "16f87fbec2f6b5cc1d84f90a16e9874493eb06a685104386cc79cc5ef1c46e75",
            "model_bytes": 35041945,
            "tags": [
                "vision",
                "body analysis",
                "emotion ferplus"
            ],
            "model_with_data_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-2.tar.gz",
            "model_with_data_sha": "07dd46eb1b83213f6fb743de869e60fdefe7f486a4d308f7612b67b046b3b013",
            "model_with_data_bytes": 32380999
        }
    },
    {
        "model": "Emotion FERPlus",
        "model_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "f800ecfe8a41e6ebf675f5830146e1d0c5c491cd2292a3ef3494c901963d3e52",
            "model_bytes": 35040571,
            "tags": [
                "vision",
                "body analysis",
                "emotion ferplus"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            64,
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus692_Output_0",
                        "shape": [
                            1,
                            8
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-7.tar.gz",
            "model_with_data_sha": "6a0341e6f1ebcad345c42883c84591e219d751590d5fdb82d4b35f09722f22ce",
            "model_with_data_bytes": 32384236
        }
    },
    {
        "model": "Emotion FERPlus",
        "model_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "a2a2ba6a335a3b29c21acb6272f962bd3d47f84952aaffa03b60986e04efa61c",
            "model_bytes": 35040571,
            "tags": [
                "vision",
                "body analysis",
                "emotion ferplus"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            64,
                            64
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus692_Output_0",
                        "shape": [
                            1,
                            8
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/emotion_ferplus/model/emotion-ferplus-8.tar.gz",
            "model_with_data_sha": "f200563a2cad231634970bb603ba74f68f01aab0f33a92251ec6d390c1991b1d",
            "model_with_data_bytes": 32384240
        }
    },
    {
        "model": "version-RFB-320",
        "model_path": "vision/body_analysis/ultraface/models/version-RFB-320.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "34cd7e60aeff28744c657de7a3dc64e872d506741de66987f3426f2b79f88017",
            "model_bytes": 1270727,
            "tags": [
                "vision",
                "body analysis",
                "ultraface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            240,
                            320
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            4420,
                            2
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "boxes",
                        "shape": [
                            1,
                            4420,
                            4
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/ultraface/models/version-RFB-320.tar.gz",
            "model_with_data_sha": "628d0dd3e0288adb821f211e13d4e97f6d6f4527237339606732dffa6f19d381",
            "model_with_data_bytes": 2015397
        }
    },
    {
        "model": "version-RFB-640",
        "model_path": "vision/body_analysis/ultraface/models/version-RFB-640.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "8f4c659275977e7a3bfbfa339a9c769ad793df50f9c0baa8c14b11baa1646430",
            "model_bytes": 1588012,
            "tags": [
                "vision",
                "body analysis",
                "ultraface"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            480,
                            640
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            17640,
                            2
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "boxes",
                        "shape": [
                            1,
                            17640,
                            4
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/body_analysis/ultraface/models/version-RFB-640.tar.gz",
            "model_with_data_sha": "07cc0e284b7924bd89f2ec103254ef3ab4b673fd12341153faaff07f3c1137e3",
            "model_with_data_bytes": 4818743
        }
    },
    {
        "model": "AlexNet-int8",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "d53bbedf100be79277cf55d78c72bdcb67d88786988561bf5d530f038e443c7b",
            "model_bytes": 60984008,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-12-int8.tar.gz",
            "model_with_data_sha": "6ec09e19e5475c51fda2d6b6205f194969ff07199e19944934b806414b34ef2e",
            "model_with_data_bytes": 40683138
        }
    },
    {
        "model": "AlexNet-qdq",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-12-qdq.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "c103c0f7b8f9d8d64b400e0899d5934d2a13d1f08368ba1907285aa0691f4cbb",
            "model_bytes": 61009072,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-12-qdq.tar.gz",
            "model_with_data_sha": "930eeb3f4d25a20e59fa21919f95edbb7a4b3f31b51a83f62c9b6627906156b9",
            "model_with_data_bytes": 45565042
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "6b0da9510f27f234ee96a85dc3809dd210150fdefad87f6850f9788513216945",
            "model_bytes": 243863787,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-12.tar.gz",
            "model_with_data_sha": "24abe33e5587bf8df8f824559d4b8c0d24db860efbe21152dc9536451ce8ddc1",
            "model_with_data_bytes": 226657165
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "e9e222ef55ba873b219f22987130fb0567cb7bc7d3f1afeb47ff442bc0e11b6d",
            "model_bytes": 243863514,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-3.tar.gz",
            "model_with_data_sha": "b640cc797d25aa446cebefaa3274d517f0c25e26d66eb6f6686ac7e4d4d8d584",
            "model_with_data_bytes": 229606515
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "f1ae53b93d83dd74731c84337ae64035288ce336794a46e2b17b56e7566feaab",
            "model_bytes": 243863628,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-6.tar.gz",
            "model_with_data_sha": "7afb4036281dfbcb33edec2fc3928cc9da5e015aee9142c03faa81b3f8481db3",
            "model_with_data_bytes": 229606527
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "d9c9134b90c760d4231e89ad9758bcc8a9397c3ba1cd56fadbef888b534ac6b4",
            "model_bytes": 243863542,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-7.tar.gz",
            "model_with_data_sha": "d3b909372a704d0c8614998b2c036d007f658e9cf1e4698a2a0e9f180d44ae20",
            "model_with_data_bytes": 226848956
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "9a4dc22fd706a19462aaec3b08e027e0ef89daf63d123824f20b59b6de9a727b",
            "model_bytes": 243863542,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-8.tar.gz",
            "model_with_data_sha": "2d651095a6a61a2c807a2ddc4bca1063224f5bcf289bb41a2c5ad3cb32728752",
            "model_with_data_bytes": 226849065
        }
    },
    {
        "model": "AlexNet",
        "model_path": "vision/classification/alexnet/model/bvlcalexnet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "ef3c82dad5c512f969dbf701fe35c0b3feb1585687d541f765eb68730c0a8af7",
            "model_bytes": 243863542,
            "tags": [
                "vision",
                "classification",
                "alexnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/alexnet/model/bvlcalexnet-9.tar.gz",
            "model_with_data_sha": "8ade7829c8705846f8f00ef67e62cf8915f23868b0f0cdc8ade19f8df2891743",
            "model_with_data_bytes": 226848864
        }
    },
    {
        "model": "CaffeNet-int8",
        "model_path": "vision/classification/caffenet/model/caffenet-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "ee44f74d2582aa4ca7131929cd28b2a07f5f39f951d8d4f997fffcbed42aa61d",
            "model_bytes": 60984071,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-12-int8.tar.gz",
            "model_with_data_sha": "24deead9c27254631332923c021c6b9cb429ef19b3367a0cc1cb171b9375c0e5",
            "model_with_data_bytes": 40718510
        }
    },
    {
        "model": "CaffeNet-qdq",
        "model_path": "vision/classification/caffenet/model/caffenet-12-qdq.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "ff1170e8123aa7b8d652bc43a9e9864ee6ba050653470f34dada9b6ad0731b40",
            "model_bytes": 61009797,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-12-qdq.tar.gz",
            "model_with_data_sha": "7d78cb5357b90e71f38c133715e67237c5156e21be0aa6e397c9654557364a6d",
            "model_with_data_bytes": 45685092
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "ea563e5c47d1f58e1717cf1a2d8bf288b8adc07af0ad228b622096cd243aeecb",
            "model_bytes": 243863798,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-12.tar.gz",
            "model_with_data_sha": "cd6077af2d35872550f2a118dc9505546eb8d80ab343f264aa90dafe5869b5c1",
            "model_with_data_bytes": 226654456
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "e362cbd353e7a0432d46ec608bfc38646aa8d70207ddb92788008816603ce835",
            "model_bytes": 243863525,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-3.tar.gz",
            "model_with_data_sha": "d6a210e64d1964f8132cbe9ea46d4ab8e7fcf2634ce4285b701deb3fe3e7dd1c",
            "model_with_data_bytes": 229550734
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "15e568474e1c2af94cc34b812628707fee0130f3a3c8b3329cbc5e4ba9248682",
            "model_bytes": 243863639,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-6.tar.gz",
            "model_with_data_sha": "bf91013f28e461722dc47acfc1ebd8464c8320d4276902204a1d810a6dd2ff2a",
            "model_with_data_bytes": 229550795
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "a249fe5683aa6b517dfb198e4b3bd40a1b7611d4995fde0b0b9ced5f4143bc5c",
            "model_bytes": 243863553,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-7.tar.gz",
            "model_with_data_sha": "6fe316230146dd17ecbf39704a9dd0c8e00f9334f804f53825d6960941fc7e09",
            "model_with_data_bytes": 226844168
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "82f8997778cd9c5ef9673fc86e340f86e3b29650886b923e5e1aa478d5f83f92",
            "model_bytes": 243863553,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-8.tar.gz",
            "model_with_data_sha": "f9b4938e7979d512b1761ad1f61879033711bd9ca609c60508c4817ee650715f",
            "model_with_data_bytes": 226844337
        }
    },
    {
        "model": "CaffeNet",
        "model_path": "vision/classification/caffenet/model/caffenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "802138332cf3e423c82a9a80c76e4f9227dd4effb3aba55413601fa4345cfd36",
            "model_bytes": 243863553,
            "tags": [
                "vision",
                "classification",
                "caffenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/caffenet/model/caffenet-9.tar.gz",
            "model_with_data_sha": "f5d1ac993a9865dc4f174f92420299a28c48279f2a2dec62b843c29b24ae7f93",
            "model_with_data_bytes": 226844131
        }
    },
    {
        "model": "DenseNet-121-12-int8",
        "model_path": "vision/classification/densenet-121/model/densenet-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "22ab62382b43c2757d84832bf7db13569ab22073d2b1c086ab5669be46d89016",
            "model_bytes": 8901304,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-12-int8.tar.gz",
            "model_with_data_sha": "d6f8fc9bf3560fb10c176e38ab49fa1e58e5aabd24362ee96d9b8635264c1285",
            "model_with_data_bytes": 6329146
        }
    },
    {
        "model": "DenseNet-121-12",
        "model_path": "vision/classification/densenet-121/model/densenet-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "0294e7e88e5b3360de9b0fdc321baf9e6ef18b7f058c4536caae3b9f18ed9ed5",
            "model_bytes": 32726877,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-12.tar.gz",
            "model_with_data_sha": "055c8ea3bdb3536063bc71b8bb3752f9482ed368a1d48c59a9fd769ded565682",
            "model_with_data_bytes": 30882195
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "7749c7b6c419cf2b8c6b8a979c86bb3d322976578682662b852cae36e2b79b41",
            "model_bytes": 32718955,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-3.tar.gz",
            "model_with_data_sha": "70586d81f49ad3bbe733270a1a58b5235794d51da3f9302cbc1e634accb6113d",
            "model_with_data_bytes": 33658961
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "875fb597a9f0f79cca8a9db610a355920b13959fbe7dc35b759dd35f4d25e291",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-6.tar.gz",
            "model_with_data_sha": "3b6401ac4491f8c4c884e8fc52dfebfe68c81ef7cd8ca0228e999b8c38284c60",
            "model_with_data_bytes": 30902760
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "79c3bc0974696a2ea9efd2f7bca19fdd630834bd0086f1b4a1c3db3dce3b2a51",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-7.tar.gz",
            "model_with_data_sha": "c8da317719041caea1e4ded47da287f604e898ec056e36ba1fa15620ec4a19ed",
            "model_with_data_bytes": 30902681
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "c57fd1e5864ddc808a84567d0d3a1ea6b5e4d1afbec17372723503be7a030e3a",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-8.tar.gz",
            "model_with_data_sha": "645ce789ac537da24b733d758089ff1b32171775ff277515177c072c3fdbb213",
            "model_with_data_bytes": 30902606
        }
    },
    {
        "model": "DenseNet-121",
        "model_path": "vision/classification/densenet-121/model/densenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "875fb597a9f0f79cca8a9db610a355920b13959fbe7dc35b759dd35f4d25e291",
            "model_bytes": 32719461,
            "tags": [
                "vision",
                "classification",
                "densenet-121"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc6_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/densenet-121/model/densenet-9.tar.gz",
            "model_with_data_sha": "3794ac95cbdbd4e5588ad5db9bdcb4f19e6cee89fdcdbc343b08a474df425d59",
            "model_with_data_bytes": 30902653
        }
    },
    {
        "model": "EfficientNet-Lite4-int8",
        "model_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11-int8.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "2b3cbb5077262b20df565dacddecb3724c0976c35029a87e512d13aa4eff04a2",
            "model_bytes": 13585963,
            "tags": [
                "vision",
                "classification",
                "efficientnet-lite4"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "images:0",
                        "shape": [
                            1,
                            224,
                            224,
                            3
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Softmax:0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11-int8.tar.gz",
            "model_with_data_sha": "d3f50f4562e61d870a70e68157ccf64d8f03f6927540ad343e6fbddb79647e1f",
            "model_with_data_bytes": 12789970
        }
    },
    {
        "model": "EfficientNet-Lite4-qdq",
        "model_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11-qdq.onnx",
        "onnx_version": "1.10.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "6837d0b19625d4aff8266d7197a7f3775afd82a8c40f9fd0283d52db4955566f",
            "model_bytes": 13469992,
            "tags": [
                "vision",
                "classification",
                "efficientnet-lite4"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "images:0",
                        "shape": [
                            1,
                            224,
                            224,
                            3
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Softmax:0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11-qdq.tar.gz",
            "model_with_data_sha": "4802f078c57e328004e326623c3de9e0e9046d9ce485eb79ed05d4d3c478b417",
            "model_with_data_bytes": 10194220
        }
    },
    {
        "model": "EfficientNet-Lite4",
        "model_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11.onnx",
        "onnx_version": "1.7.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "d111689907c06eea7c82e4833ddef758da6453b9d4cf60b7e99ca05c7cbd9c12",
            "model_bytes": 51946641,
            "tags": [
                "vision",
                "classification",
                "efficientnet-lite4"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "images:0",
                        "shape": [
                            1,
                            224,
                            224,
                            3
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Softmax:0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "efficientnet-lite4/model/head/AvgPool:0",
                        "shape": [
                            1,
                            1280,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/efficientnet-lite4/model/efficientnet-lite4-11.tar.gz",
            "model_with_data_sha": "1914de663a575f0ec015b7a33c335b363e910f64cc91d08e66587ea877fd9d7b",
            "model_with_data_bytes": 48592826
        }
    },
    {
        "model": "GoogleNet-int8",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "282290386d7055c1b1e9692680621907991a87f9839391892b5cbe4da2e9871f",
            "model_bytes": 7122858,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-12-int8.tar.gz",
            "model_with_data_sha": "fcedb191d6fd2453ed27f1dbf192cda8b88ec83a6d2a9448fa1d24a2c23cd675",
            "model_with_data_bytes": 5724344
        }
    },
    {
        "model": "GoogleNet-qdq",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-12-qdq.onnx",
        "onnx_version": "1.12",
        "opset_version": 12,
        "metadata": {
            "model_sha": "f764ae1ed52e5fca319a43a19e9526d2f028e7f10b9d17a8eefcb098cd90d36d",
            "model_bytes": 7135204,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-12-qdq.tar.gz",
            "model_with_data_sha": "5587255e94438edd7c0017f5355178546b150deea05ed5e59109a4d4ae9aa4b3",
            "model_with_data_bytes": 5562451
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "c99c507058eaf41de8723408fdda7db8325cb57f0a89f2ee07a716d6e963e14e",
            "model_bytes": 28021836,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-12.tar.gz",
            "model_with_data_sha": "cdc6457821113082defd34905342385f63f823363feb1bf439eee4427577659c",
            "model_with_data_bytes": 26545491
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "5725ccfd90845735ce5897faf7843c3f9b13acda3c4f3bff82cb21c9337c9de8",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-3.tar.gz",
            "model_with_data_sha": "26348e6f4e8073a8b6bf878b4104941443fff445ffa5785b1ae66ad955f209a8",
            "model_with_data_bytes": 26565157
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "bd979d99d10d9acf84c5a67252bcae4992bf4e1afdab046cb0ea370e6f01ee38",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-6.tar.gz",
            "model_with_data_sha": "3085e172e8cd28e049e9d47ae575f8063fff6c98a85b056f89158e8b2fd39018",
            "model_with_data_bytes": 26565143
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "5725ccfd90845735ce5897faf7843c3f9b13acda3c4f3bff82cb21c9337c9de8",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-7.tar.gz",
            "model_with_data_sha": "58219a44c0a22a95aa4956d653f7688a753cb9389fbf08bbcad6c62b0294c6fd",
            "model_with_data_bytes": 26565243
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "3d343d459199fbcc8cd43d45264180e3e1ef91d4908f3541b5b308995206362c",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-8.tar.gz",
            "model_with_data_sha": "5f1020194e63177e0765aa8de917b4e7057cd3c62bb46b4fe4564210cdabe3bc",
            "model_with_data_bytes": 26565142
        }
    },
    {
        "model": "GoogleNet",
        "model_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "bd979d99d10d9acf84c5a67252bcae4992bf4e1afdab046cb0ea370e6f01ee38",
            "model_bytes": 28020232,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "googlenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/googlenet/model/googlenet-9.tar.gz",
            "model_with_data_sha": "ea5a81dd36c3067a78cfc1e6baa7a4142849e28d97d054fdc42aa1f9539d400f",
            "model_with_data_bytes": 26565214
        }
    },
    {
        "model": "Inception-1-int8",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "7d25f0bfa7a92e9a67d8049facd4454911ca4388a303fc61ee14664b1010c717",
            "model_bytes": 10191535,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12-int8.tar.gz",
            "model_with_data_sha": "bc3d78134d9777d3e3a3320523834beb2ba7bdd1c5e017cba5a8d268691afbc6",
            "model_with_data_bytes": 9474526
        }
    },
    {
        "model": "Inception-1-qdq",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12-qdq.onnx",
        "onnx_version": "1.12",
        "opset_version": 12,
        "metadata": {
            "model_sha": "c6a8e07c53ea417a0001ebba885a3a623f1c3c51fac105485aca638e2f3de25d",
            "model_bytes": 7135424,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12-qdq.tar.gz",
            "model_with_data_sha": "c981fd944a125d3626cb79720c7f0a57309da5a24994c0378e2657fdca87a869",
            "model_with_data_bytes": 5559367
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "814b31a6316873bd03d2d808415ce1f40a7bf48ad88eab5ecc3cdc8dc6cf38ec",
            "model_bytes": 28021958,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-12.tar.gz",
            "model_with_data_sha": "c4198ed3415d59dacab2f6fa90c81f1a3aa9f3623b66e2d9653f16d06a5cc68c",
            "model_with_data_bytes": 26545542
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "6e6a6091a76dcc34b7afd68cec9355687278844b92fb0364b608e5c6f33067e8",
            "model_bytes": 28020217,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-3.tar.gz",
            "model_with_data_sha": "18dd953ae8f2056ac0a1fd56302ef2a5d56b35ccee44fddc7bbdcbcfe9a5adf1",
            "model_with_data_bytes": 29324028
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "9e7c4e026430fc055cb040dfe7f7978556b3af13cd618a7866841c00dd6d178f",
            "model_bytes": 28020390,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-6.tar.gz",
            "model_with_data_sha": "caf55082c6605ff17e0074e7af862f774b6efb00557236676e7112f6f54cec61",
            "model_with_data_bytes": 29324092
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "eaa5c289961ef2134290ddd42852d818d1d40fce18ffbc876bc446483343af9c",
            "model_bytes": 28020356,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-7.tar.gz",
            "model_with_data_sha": "56c2d167a5400861d674c8a3d73df3fb3c4007b40ef350edf4da5e900138f86b",
            "model_with_data_bytes": 26565189
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "ec956d393dd467b6d9e0b17b8c8227c534b936bfbcdc82343a3a7238375775f0",
            "model_bytes": 28020356,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-8.tar.gz",
            "model_with_data_sha": "e3ecd30178504237bbf45dc87ae96a46b6301157a8b8e5c8cf25c817e4512628",
            "model_with_data_bytes": 26565228
        }
    },
    {
        "model": "Inception-1",
        "model_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "f8b7bb4040377b83089eef98800a15303325ed75ced468a8972a87e0bc23692e",
            "model_bytes": 28020356,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_2",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v1/model/inception-v1-9.tar.gz",
            "model_with_data_sha": "faeb7ab80e0f60d22ae8472f6188700a20cd3fc466682f2dab5e00e8955c611c",
            "model_with_data_bytes": 26565173
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "8bb3ba787e84667bc1169034c36a1f1c1cddfbd19d5e48c9c51306a087973bc8",
            "model_bytes": 45042595,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-3.tar.gz",
            "model_with_data_sha": "37ea826b395c559ed2cc148823ea77f793e591c5eff5a718d98733793b680dbc",
            "model_with_data_bytes": 45015362
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "53626ce95a96756469d23b8dd917f8b5a6015c628369d219bdbf17bb5d0178f5",
            "model_bytes": 45040501,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-6.tar.gz",
            "model_with_data_sha": "a29498bbf22d78dc428f0b6a0ecdc350475896ccb653c0b8d544c14000509b0a",
            "model_with_data_bytes": 45015338
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "e1ada57696a4d2c23984b8584cd8df40243301f7344475a8653118c0d5da57a5",
            "model_bytes": 45042799,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-7.tar.gz",
            "model_with_data_sha": "5ef4d35e4490468986254a5915a567849183b211b49c2552e816165a74dd4aa0",
            "model_with_data_bytes": 42257087
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "878e9c04172215cb7a1ce225dfecfe2ed098abde5cb1067dc644a3629ec6dcf1",
            "model_bytes": 45042799,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-8.tar.gz",
            "model_with_data_sha": "c222c2f1c530aad79496f0b15898d4aa6807dc66e82ac67cec1573cfd4b59c87",
            "model_with_data_bytes": 42257042
        }
    },
    {
        "model": "Inception-2",
        "model_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "37ed4aa93f817fd0d1cc19bda045ebd70f18d0887a5f51e66ecd0ed8fd5a65c2",
            "model_bytes": 45042799,
            "tags": [
                "vision",
                "classification",
                "inception and googlenet",
                "inception v2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool5/7x7_s1_1",
                        "shape": [
                            1,
                            1024,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/inception_and_googlenet/inception_v2/model/inception-v2-9.tar.gz",
            "model_with_data_sha": "c0722dff7401d4ade189c62f66b3a39eaa3b15c1a28457b7e3efd44de9917491",
            "model_with_data_bytes": 42256996
        }
    },
    {
        "model": "MNIST",
        "model_path": "vision/classification/mnist/model/mnist-1.onnx",
        "onnx_version": "1.0",
        "opset_version": 1,
        "metadata": {
            "model_sha": "22239f3fcc38f34d02eecd6869aed15b93f8e3e1125dda48990d244a5e113d49",
            "model_bytes": 27266,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "model_with_data_path": "vision/classification/mnist/model/mnist-1.tar.gz",
            "model_with_data_sha": "a296485a657137a78b5c9b0ade09fca4041364da2c530dd0edfca1e1bb53283e",
            "model_with_data_bytes": 26518
        }
    },
    {
        "model": "MNIST-12-int8",
        "model_path": "vision/classification/mnist/model/mnist-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "5eac0752eecf4991a5e4157cf31a186f2c61090494019e87979ac17df1dae253",
            "model_bytes": 10969,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus214_Output_0",
                        "shape": [
                            1,
                            10
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mnist/model/mnist-12-int8.tar.gz",
            "model_with_data_sha": "e4ea757f0fbf49c5aebc0c885397c72dc7a38617fbef2db408f531f91a7bb49c",
            "model_with_data_bytes": 10668
        }
    },
    {
        "model": "MNIST-12",
        "model_path": "vision/classification/mnist/model/mnist-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "5c688690f8bacf667d4c2074af5ad0646ca328d7ab03eccf944a65b320171bdd",
            "model_bytes": 26143,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus214_Output_0",
                        "shape": [
                            1,
                            10
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mnist/model/mnist-12.tar.gz",
            "model_with_data_sha": "a53a59dcaca8804a0f6dfda9a3cf2e082979589391dbde73640b60684f1d24e9",
            "model_with_data_bytes": 26741
        }
    },
    {
        "model": "MNIST",
        "model_path": "vision/classification/mnist/model/mnist-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "0d715376572e89832685c56a65ef1391f5f0b7dd31d61050c91ff3ecab16c032",
            "model_bytes": 26454,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus214_Output_0",
                        "shape": [
                            1,
                            10
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mnist/model/mnist-7.tar.gz",
            "model_with_data_sha": "35601ad2a49200f62e3030d1530e5db1c1edeb2d6632b2216bcebee2287dbb7c",
            "model_with_data_bytes": 26757
        }
    },
    {
        "model": "MNIST",
        "model_path": "vision/classification/mnist/model/mnist-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "2f06e72de813a8635c9bc0397ac447a601bdbfa7df4bebc278723b958831c9bf",
            "model_bytes": 26454,
            "tags": [
                "vision",
                "classification",
                "mnist"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "Input3",
                        "shape": [
                            1,
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Plus214_Output_0",
                        "shape": [
                            1,
                            10
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mnist/model/mnist-8.tar.gz",
            "model_with_data_sha": "2f47e338faddbb30488abd83e8179ce44882de465165bb3a5b2640719f64f70c",
            "model_with_data_bytes": 26751
        }
    },
    {
        "model": "MobileNet v2-1.0",
        "model_path": "vision/classification/mobilenet/model/mobilenetv2-10.onnx",
        "onnx_version": "1.5.0",
        "opset_version": 10,
        "metadata": {
            "model_sha": "0e7c0aa4bc74650386fa1d2c84705753de7c2bdb21909ada5c59154bb429e092",
            "model_bytes": 13963115,
            "tags": [
                "vision",
                "classification",
                "mobilenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "464",
                        "shape": [
                            0,
                            1280,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mobilenet/model/mobilenetv2-10.tar.gz",
            "model_with_data_sha": "3e4e9e2de0241cc829917e9c0233229974f5d35d3cfd012a263ca36c98f36ac3",
            "model_with_data_bytes": 13504391
        }
    },
    {
        "model": "MobileNet v2-1.0-int8",
        "model_path": "vision/classification/mobilenet/model/mobilenetv2-12-int8.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "cc028fe6cae7bc11a4ff53cfc9b79c920e8be65ce33a904ec3e2a8f66d77f95f",
            "model_bytes": 3655033,
            "tags": [
                "vision",
                "classification",
                "mobilenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mobilenet/model/mobilenetv2-12-int8.tar.gz",
            "model_with_data_sha": "dfcf1dc15b78611489f22a19902de12e304bf0380e185bde499fcf70f7647455",
            "model_with_data_bytes": 3914892
        }
    },
    {
        "model": "MobileNet v2-1.0-qdq",
        "model_path": "vision/classification/mobilenet/model/mobilenetv2-12-qdq.onnx",
        "onnx_version": "1.10.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "41a36090dafe98f4ad8f9b7fe0b218c56ac3c031e547f0367c30655d2702bffe",
            "model_bytes": 3593903,
            "tags": [
                "vision",
                "classification",
                "mobilenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mobilenet/model/mobilenetv2-12-qdq.tar.gz",
            "model_with_data_sha": "dd8cafda6fdf773a37ba09858482232eca8e01997ec6f1bebf1aca11aea0c1ce",
            "model_with_data_bytes": 3434401
        }
    },
    {
        "model": "MobileNet v2-1.0-fp32",
        "model_path": "vision/classification/mobilenet/model/mobilenetv2-12.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "c0c3f76d93fa3fd6580652a45618618a220fced18babf65774ed169de0432ad5",
            "model_bytes": 13964571,
            "tags": [
                "vision",
                "classification",
                "mobilenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mobilenet/model/mobilenetv2-12.tar.gz",
            "model_with_data_sha": "5f83b422708a708b592a5fd56d84710038511789df3ae510b84123930c37762d",
            "model_with_data_bytes": 13498787
        }
    },
    {
        "model": "MobileNet v2-7",
        "model_path": "vision/classification/mobilenet/model/mobilenetv2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "c1c513582d56afceff8516c73804e484c81c6a830712ab6d682253f4a3cd042f",
            "model_bytes": 14246826,
            "tags": [
                "vision",
                "classification",
                "mobilenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "mobilenetv20_output_flatten0_reshape0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/mobilenet/model/mobilenetv2-7.tar.gz",
            "model_with_data_sha": "b463ad62dae99f13afd88549ca7d43e9bda6876614f3592ebb41177e1db0fcc5",
            "model_with_data_bytes": 13682646
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "bc0f63a5f40bad37092c456cac821a99ee63a0f5a453f045e4b39d25823ddbdb",
            "model_bytes": 230753133,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-3.tar.gz",
            "model_with_data_sha": "eb4fab60b84e2c69dc634f91bd669867ac7ef8828499a28023472df3219ac59c",
            "model_with_data_bytes": 217332838
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "d6bbbd835d0e0654b0fceda3b186505a4d89fd8cf0282f403035823d0bdefa8f",
            "model_bytes": 230753247,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-6.tar.gz",
            "model_with_data_sha": "399c2629de63824d52b5f71db731995492f4ebb8a2892fa36236925a4bad9f44",
            "model_with_data_bytes": 217332967
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "46cf29e309bb65b9f754839aa214f3c13caddccd3204178d9034ce091c4bfd17",
            "model_bytes": 230753161,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc-rcnn_1",
                        "shape": [
                            1,
                            200
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-7.tar.gz",
            "model_with_data_sha": "a9c56a0e02b4df95a1639f6d7883e39b9f416f8929d09cdf6f45783aa1605d1f",
            "model_with_data_bytes": 214642762
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "eedeefa170e2d541c6bbd60a935f79396f56721aef1cfdfe79e767ed536deb85",
            "model_bytes": 230753161,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc-rcnn_1",
                        "shape": [
                            1,
                            200
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-8.tar.gz",
            "model_with_data_sha": "ce469bf52e1a8bf2884f502e8921ce2e7cd92f9aeb7d9a16c6da639cf8d39a74",
            "model_with_data_bytes": 214642707
        }
    },
    {
        "model": "R-CNN ILSVRC13",
        "model_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "4aadb5ac806e82f89bac7dea77c838ba828dccbe5d624d8a44ebd513953571d9",
            "model_bytes": 230753161,
            "tags": [
                "vision",
                "classification",
                "rcnn ilsvrc13"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "fc-rcnn_1",
                        "shape": [
                            1,
                            200
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_1",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/rcnn_ilsvrc13/model/rcnn-ilsvrc13-9.tar.gz",
            "model_with_data_sha": "ef3f40805e4257213440401536e28ad3aa6404ebce5a6cf9d7fcdb44fee09f37",
            "model_with_data_bytes": 214642550
        }
    },
    {
        "model": "ResNet101",
        "model_path": "vision/classification/resnet/model/resnet101-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "760702f6ec0138f71ebac66f4e95d1c72dc9267a8544f4d4667a53075fd0497c",
            "model_bytes": 178914043,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv18_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv18_pool1_fwd",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet101-v1-7.tar.gz",
            "model_with_data_sha": "27f51a51a9de6c1b10455cb09164f384746288c4b9e7bbb4af49ab206abd6703",
            "model_with_data_bytes": 166222423
        }
    },
    {
        "model": "ResNet101-v2",
        "model_path": "vision/classification/resnet/model/resnet101-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "3a0a819ea0953fb4700b0d98e43540aeee253e5b4c503c552b0c7979fc67767d",
            "model_bytes": 178682301,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv25_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv25_pool1_fwd",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet101-v2-7.tar.gz",
            "model_with_data_sha": "566663c700f5cc1e8eb0043c00923d5422abaca4fae21568973ef9c446084f6e",
            "model_with_data_bytes": 165872227
        }
    },
    {
        "model": "ResNet152",
        "model_path": "vision/classification/resnet/model/resnet152-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "b0ec92437bf8c9d0739bf9bafdea9638e13f9e3c1e2fb766ad546cc182fd3f6d",
            "model_bytes": 241816206,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv19_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv19_pool1_fwd",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet152-v1-7.tar.gz",
            "model_with_data_sha": "470ccde11c3ee2b490c214e48459c03e5b4118e8143f83dd795263efcf794cf3",
            "model_with_data_bytes": 226450954
        }
    },
    {
        "model": "ResNet152-v2",
        "model_path": "vision/classification/resnet/model/resnet152-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "40fb3f6dd313ba786a2c8077a0ef7d7b4afeda5d037365400d3a7fbc2b1bee64",
            "model_bytes": 241503848,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv27_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv27_pool1_fwd",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet152-v2-7.tar.gz",
            "model_with_data_sha": "78b507483050a427870a264c398d121f44b92d69c8ca0c53217eb5137b6b51e3",
            "model_with_data_bytes": 225131715
        }
    },
    {
        "model": "ResNet18",
        "model_path": "vision/classification/resnet/model/resnet18-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "4e8f8653e7a2222b3904cc3fe8e304cd8b339ce1d05fd24688162f86fb6df52c",
            "model_bytes": 46820737,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv15_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv15_pool1_fwd",
                        "shape": [
                            0,
                            512,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet18-v1-7.tar.gz",
            "model_with_data_sha": "2163225f89c858f120b5376758b752e57139601e967f6c6f614a3d77e9d66505",
            "model_with_data_bytes": 43835620
        }
    },
    {
        "model": "ResNet18-v2",
        "model_path": "vision/classification/resnet/model/resnet18-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "3b4d048057854022232aa20bf68262e31718ab7119a17f920b8a2dc1433f824b",
            "model_bytes": 46806737,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv22_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv22_pool1_fwd",
                        "shape": [
                            0,
                            512,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet18-v2-7.tar.gz",
            "model_with_data_sha": "a64c111fac5d4e8bebf9246507ec7a92ea1c1596de09fca73bb7f94e9750e316",
            "model_with_data_bytes": 43808776
        }
    },
    {
        "model": "ResNet34",
        "model_path": "vision/classification/resnet/model/resnet34-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "883a41b2e30ddc2e422c108b091b5e70b92d37dc5c345a2c563dcae27971e2cc",
            "model_bytes": 87302588,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv16_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv16_pool1_fwd",
                        "shape": [
                            0,
                            512,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet34-v1-7.tar.gz",
            "model_with_data_sha": "c95b29c28e4d2c6b9ee7969e668b35cff335ec11761e9722f3ebe67490c19f7b",
            "model_with_data_bytes": 81220173
        }
    },
    {
        "model": "ResNet34-v2",
        "model_path": "vision/classification/resnet/model/resnet34-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "3d3815e25f4b1ea967e2a526a3b27ef2a03af1725bdc199a63a271c26862d914",
            "model_bytes": 87288587,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv23_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv23_pool1_fwd",
                        "shape": [
                            0,
                            512,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet34-v2-7.tar.gz",
            "model_with_data_sha": "b04865581abb9a51af87cfc170ac26e928eeade336e16abbe984c410441453cc",
            "model_with_data_bytes": 81189514
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "22cbd5723582d9e4636c46da43732980c8a60f8fc74c37f15db522034e6ae84d",
            "model_bytes": 102491873,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-3.tar.gz",
            "model_with_data_sha": "b83fbf3bc6a25ece62216e2ed801061383af864c1b1ddc9c18d0a08e6daead61",
            "model_with_data_bytes": 96754305
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "166b367d86f2afe34f7438154f2798d081b13d369df20c75b485b76763c16a0e",
            "model_bytes": 102490291,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-6.tar.gz",
            "model_with_data_sha": "92df484b9242282bb3a8ee6e82849db3c82dd96d231f253c9287b483c6f119e5",
            "model_with_data_bytes": 98379563
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "48c377bd20b8e7c1b4a2e371c1badefbb9db906f57bcc5c800a28835c6b988bf",
            "model_bytes": 102489425,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/pool5_1",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-7.tar.gz",
            "model_with_data_sha": "c3c19b61bffe72bf94b6c0214c1f5f5375b3cca08ee24ec83fff515b6761ac57",
            "model_with_data_bytes": 95636370
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "4f4a244f1485883fbf4faa0f967409a747164733a7b59b2d07d124d6bbd26ae0",
            "model_bytes": 102489425,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/pool5_1",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-8.tar.gz",
            "model_with_data_sha": "04b0525962974065137fcf3d37cb4608d73f36e3d28bea5933af56e61cad35ce",
            "model_with_data_bytes": 95636234
        }
    },
    {
        "model": "ResNet50-caffe2",
        "model_path": "vision/classification/resnet/model/resnet50-caffe2-v1-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "822daa93e11ecd505f282eb16e02d7fa6d7a88a4db0c9f832320798e508ba0dd",
            "model_bytes": 102489425,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/pool5_1",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-caffe2-v1-9.tar.gz",
            "model_with_data_sha": "e0703ef13fefd8207747ec5713eb07fc87fa0ab1257e92a052bc7b761101d49c",
            "model_with_data_bytes": 95636091
        }
    },
    {
        "model": "ResNet50-int8",
        "model_path": "vision/classification/resnet/model/resnet50-v1-12-int8.onnx",
        "onnx_version": "1.7.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "c234f30975989788b4405f25253275aae247ab6dbdd34aaa69ab0a59ff76f6d0",
            "model_bytes": 25816052,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv17_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v1-12-int8.tar.gz",
            "model_with_data_sha": "662710f50cafe29f44727b9cd8be5430fb06da18d3004cf5f9d9645c0dc39d71",
            "model_with_data_bytes": 22355322
        }
    },
    {
        "model": "ResNet50-qdq",
        "model_path": "vision/classification/resnet/model/resnet50-v1-12-qdq.onnx",
        "onnx_version": "1.10.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "e6429d274805654c79ba68dc886c1efd4eff95393e747c35d3fdab579febd8a0",
            "model_bytes": 25753167,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv17_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v1-12-qdq.tar.gz",
            "model_with_data_sha": "6a93ba624ecab599d3f8511b1409f5103beb2f1708a85955397a3132ca423d99",
            "model_with_data_bytes": 17049336
        }
    },
    {
        "model": "ResNet50-fp32",
        "model_path": "vision/classification/resnet/model/resnet50-v1-12.onnx",
        "onnx_version": "1.7.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "3f03fdef724b22947eed826f1eef1dc5c34151bb4c37d634f1db89dfa2dd1526",
            "model_bytes": 102576593,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv17_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v1-12.tar.gz",
            "model_with_data_sha": "9391137cfc8fbec372d7a1e59e272d67550dab72d93cf7c7d6256782262599ea",
            "model_with_data_bytes": 96559469
        }
    },
    {
        "model": "ResNet50",
        "model_path": "vision/classification/resnet/model/resnet50-v1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "af16a04a6ec48ac494065d4439fe9dea590d337b9ca6dc328160ccf04a217b9c",
            "model_bytes": 102583340,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv17_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv17_pool1_fwd",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v1-7.tar.gz",
            "model_with_data_sha": "898a9183256e884ae06fb1c11869386eccd38393ab41d9339909e974519a9feb",
            "model_with_data_bytes": 95435189
        }
    },
    {
        "model": "ResNet50-v2",
        "model_path": "vision/classification/resnet/model/resnet50-v2-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "79102261eb6e5fd7af5d27f41316293e388c5cb691e5d25bfb035c4f64fefe31",
            "model_bytes": 102442452,
            "tags": [
                "vision",
                "classification",
                "resnet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            "N",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "resnetv24_dense0_fwd",
                        "shape": [
                            "N",
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "resnetv24_pool1_fwd",
                        "shape": [
                            0,
                            2048,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/model/resnet50-v2-7.tar.gz",
            "model_with_data_sha": "d6a8b7ec924dae64e19c0850e7aa9c633095f7c4422393f5cc478c4982e97667",
            "model_with_data_bytes": 95237476
        }
    },
    {
        "model": "ResNet-preproc",
        "model_path": "vision/classification/resnet/preproc/resnet-preproc-v1-18.onnx",
        "onnx_version": "1.13.1",
        "opset_version": 18,
        "metadata": {
            "model_sha": "9cda24af90b4cd2ced4167fa36a41956ea0ce5e55c6ae475614a097cb89762c7",
            "model_bytes": 1129,
            "tags": [
                "vision",
                "classification",
                "resnet",
                "preprocessing"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "images",
                        "shape": [],
                        "type": "seq(tensor(uint8))"
                    }
                ],
                "outputs": [
                    {
                        "name": "preproc_data",
                        "shape": [
                            "B",
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/resnet/preproc/resnet-preproc-v1-18.tar.gz",
            "model_with_data_sha": "216b89c1676c8a5a2dfc0ee1736b179b0777f9ba845ee6dd955d4ff684f29a3c",
            "model_with_data_bytes": 883999
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "ebbf0f6cf45d6971dee7833bfa9cceb868f19ed753c5bf6053857cdb68fc15bc",
            "model_bytes": 5723516,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-3.tar.gz",
            "model_with_data_sha": "eeeef9e70d858a3fd72b9eb33435e480a9f2fac0fa0f2d2c75b148fe3e62cb7f",
            "model_with_data_bytes": 6980244
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "b0e5f38aa3961e6d208ccfe5e34dc52c1b9bf96f4b377cdba1919c6810457258",
            "model_bytes": 5724572,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-6.tar.gz",
            "model_with_data_sha": "9f1c78a572dcf40f1056bdb9688433290c010c91e98ba4526be1ffdbfd854660",
            "model_with_data_bytes": 8604055
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "359609ab27d27d44091e05f9a5127d9ce0c155ed69d0716f41e6779998bc4764",
            "model_bytes": 5723770,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/final_avg_1",
                        "shape": [
                            1,
                            544,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-7.tar.gz",
            "model_with_data_sha": "323202f6de6ed3b853452967fdd4df27f034b9224c94a123d667aa14e3e96fee",
            "model_with_data_bytes": 5858831
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "faabc409fd57ae83ddebe7fb981ee492a7899c1acb4893cc8e64c11f0ccdaa98",
            "model_bytes": 5723770,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/final_avg_1",
                        "shape": [
                            1,
                            544,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-8.tar.gz",
            "model_with_data_sha": "090749323dc94018eccf6d1ae2d1ab575b6e0b24ff1f77d248e780eecf5fd61b",
            "model_with_data_bytes": 5858851
        }
    },
    {
        "model": "ShuffleNet-v1",
        "model_path": "vision/classification/shufflenet/model/shufflenet-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "3642c54024f09d35236f325df9822e772596c394cd90fd8e98ca1d2eba309e6f",
            "model_bytes": 5723770,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/final_avg_1",
                        "shape": [
                            1,
                            544,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-9.tar.gz",
            "model_with_data_sha": "75ff4f8c8cacbb09623a6b49906c9932d7f717a9df21637f0a1c63337107343f",
            "model_with_data_bytes": 5858835
        }
    },
    {
        "model": "ShuffleNet-v2",
        "model_path": "vision/classification/shufflenet/model/shufflenet-v2-10.onnx",
        "onnx_version": "1.6",
        "opset_version": 10,
        "metadata": {
            "model_sha": "8b85e969a0a5518b554c99548379d1ba54d973733a6a12012138d91852308584",
            "model_bytes": 9218554,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "611",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-v2-10.tar.gz",
            "model_with_data_sha": "3dfa8224c1233b22006b43c689eebc4af46917cc1de0bd123f43510d8ab06bf2",
            "model_with_data_bytes": 8721925
        }
    },
    {
        "model": "ShuffleNet-v2-int8",
        "model_path": "vision/classification/shufflenet/model/shufflenet-v2-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "bf5eefde67941aa7eaa7787316c9c52c4b04e07fc9587a6ea7028883be9a8d4e",
            "model_bytes": 2388912,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "611",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-v2-12-int8.tar.gz",
            "model_with_data_sha": "f538889aa028f6fcc4db1dce9b0bf2e7e347a81d5fc28087d96758dc0131e8c7",
            "model_with_data_bytes": 2488137
        }
    },
    {
        "model": "ShuffleNet-v2-qdq",
        "model_path": "vision/classification/shufflenet/model/shufflenet-v2-12-qdq.onnx",
        "onnx_version": "1.12",
        "opset_version": 12,
        "metadata": {
            "model_sha": "7c536d02e2f6af9569e5f3c7a4d8282060072119524c93c8da71e63876b4722b",
            "model_bytes": 2415805,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-v2-12-qdq.tar.gz",
            "model_with_data_sha": "071594e233cedf5688501c9b67cad30c4babb1b46771fb87afe7fd1beb1cc008",
            "model_with_data_bytes": 2245304
        }
    },
    {
        "model": "ShuffleNet-v2-fp32",
        "model_path": "vision/classification/shufflenet/model/shufflenet-v2-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "ea69821b4dd374ae2f33f9710dd1229ac263d0ee5b5a46ca3521f6483e1ba035",
            "model_bytes": 9218556,
            "tags": [
                "vision",
                "classification",
                "shufflenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "611",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/shufflenet/model/shufflenet-v2-12.tar.gz",
            "model_with_data_sha": "ec61f826d53c4630b2b4ead3514f2320a543abf0e862f6b12f91456b8188a4c5",
            "model_with_data_bytes": 9113274
        }
    },
    {
        "model": "SqueezeNet 1.0-int8",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "3da17dfad1b7ba23c93fac6dbf49f6db78cd42f7519e915a2e27d37c5c0a972b",
            "model_bytes": 1293388,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-12-int8.tar.gz",
            "model_with_data_sha": "28de6cb53cdaf15b81d293dc480b5f4f9fd766ab05bc97dbc1a5befb6981f1b2",
            "model_with_data_bytes": 1562359
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "dec81a8684617770b3cf13fadc1d92565d1d453d23935fc6388b792d99c992bd",
            "model_bytes": 4952956,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool10_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-12.tar.gz",
            "model_with_data_sha": "8a2dcc5a8f2b8c314b96b6484703a72882c796faa4eaf0de1b913fc0765cb917",
            "model_with_data_bytes": 5151210
        }
    },
    {
        "model": "SqueezeNet 1.0-qdq",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-13-qdq.onnx",
        "onnx_version": "1.9",
        "opset_version": 13,
        "metadata": {
            "model_sha": "4a567dd7542ef440890d57268fabf47211174c593d7a1837bd7f16a1067169e7",
            "model_bytes": 1345213,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-13-qdq.tar.gz",
            "model_with_data_sha": "d3a84101535d52ecf31503ff1645f170c7b156b6bc53f1492caec485b7c023a7",
            "model_with_data_bytes": 1565787
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "f68e6ad41574fa495fc28db95dd47b78a79522e305990f6f6d2a1844607f886d",
            "model_bytes": 4952238,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-3.tar.gz",
            "model_with_data_sha": "80dc32f8172209d139160258da093dc08af95e61ec4457f98b9061499020331c",
            "model_with_data_bytes": 6226586
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "513a21407cccfcc74eb20e7749de7d5dda38f3f3199fd3259deeca9c021d94a0",
            "model_bytes": 4952238,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool10_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-6.tar.gz",
            "model_with_data_sha": "9da41b593f063b70d41d1a8245983ce85ba4fe6f2402225e9e04e19fd943173a",
            "model_with_data_bytes": 6226584
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "d95e2191e056f1912a9b8f6000da3b9c7818441b9eb48137033c2adbf6398bc8",
            "model_bytes": 4952222,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool10_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-7.tar.gz",
            "model_with_data_sha": "62b373de7241b04041c688a275612a4d55507012ed8d5a077dd6c68f108909ca",
            "model_with_data_bytes": 5154452
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "3c0bff2f7c984350d3ffa98e0167dc08c15da88f05d0c31989ebeba6fbb5e025",
            "model_bytes": 4952222,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool10_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-8.tar.gz",
            "model_with_data_sha": "f08042579f6f9aaa32aefca1a111710035335fabe804036c88c54dda6606294a",
            "model_with_data_bytes": 5154407
        }
    },
    {
        "model": "SqueezeNet 1.0",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.0-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "f2d3201f013b72988109e677e9244250f4cc02394529432972f05e1f6dcc7b62",
            "model_bytes": 4952222,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "softmaxout_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "pool10_1",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.0-9.tar.gz",
            "model_with_data_sha": "210b9509fa78688b7ade604a210e03bdb76f3d0eac28eb4a79c6e9d5500342df",
            "model_with_data_bytes": 5154458
        }
    },
    {
        "model": "SqueezeNet 1.1",
        "model_path": "vision/classification/squeezenet/model/squeezenet1.1-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "1eeff551a67ae8d565ca33b572fc4b66e3ef357b0eb2863bb9ff47a918cc4088",
            "model_bytes": 4956208,
            "tags": [
                "vision",
                "classification",
                "squeezenet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "squeezenet0_flatten0_reshape0",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "squeezenet0_pool3_fwd",
                        "shape": [
                            1,
                            1000,
                            1,
                            1
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/squeezenet/model/squeezenet1.1-7.tar.gz",
            "model_with_data_sha": "95dc7fbafbc7372aabe75555db9e89f05e0715ab7472cf7f8fb751ff54543c3e",
            "model_with_data_bytes": 5156484
        }
    },
    {
        "model": "VGG 16-int8",
        "model_path": "vision/classification/vgg/model/vgg16-12-int8.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "0d0a8e1d69f70404421e37dbad4f8bd21d42758e9f426bd5b39e9e4e9e1afa23",
            "model_bytes": 138407238,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "flatten_70_quantized",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg16-12-int8.tar.gz",
            "model_with_data_sha": "75c08988b598e24173a21a7dc0b396ca9dc49afb63be35e5c993ac56b297e4cf",
            "model_with_data_bytes": 106112057
        }
    },
    {
        "model": "VGG 16-fp32",
        "model_path": "vision/classification/vgg/model/vgg16-12.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "722c42f53ca03a5480296bf8f9c85a151de3eed9ec982d7d2302ff6e2900c67b",
            "model_bytes": 553437752,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg16-12.tar.gz",
            "model_with_data_sha": "bab278d40411f29dfc2e9515c534d2871eb0393480b736855bfe7f73c312cbc6",
            "model_with_data_bytes": 511942031
        }
    },
    {
        "model": "VGG 16",
        "model_path": "vision/classification/vgg/model/vgg16-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "f20805a3ecccaa88647bbab4ad011ff2412a9838485ea844de0fcbce349820b9",
            "model_bytes": 553437328,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "flatten_70",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg16-7.tar.gz",
            "model_with_data_sha": "586413f9108e2316ebf80c3aa484d55f995d693b153e4c90742750519a9e27be",
            "model_with_data_bytes": 512679010
        }
    },
    {
        "model": "VGG 16-bn",
        "model_path": "vision/classification/vgg/model/vgg16-bn-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "4fdceabf1ef715c9ed3020fa2eac1cb2a448296c086b03633c80cf47e1c5afce",
            "model_bytes": 553512191,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "flatten_135",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg16-bn-7.tar.gz",
            "model_with_data_sha": "143e59ca3000cc3b461cfe09d4c62a93bb5e9f7a0c4e5db0010e1a525fdd50bd",
            "model_with_data_bytes": 512894836
        }
    },
    {
        "model": "VGG 19",
        "model_path": "vision/classification/vgg/model/vgg19-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "692f9f927f1df1b89033787a77a73eac59fd9b3344f9ecdf0b13221e41c2046f",
            "model_bytes": 574677321,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "flatten_82",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-7.tar.gz",
            "model_with_data_sha": "797517bc882ea820d09f803e711c203e223b0c214b3c85412cb6c059b63325b7",
            "model_with_data_bytes": 532124692
        }
    },
    {
        "model": "VGG 19-bn",
        "model_path": "vision/classification/vgg/model/vgg19-bn-7.onnx",
        "onnx_version": "1.2.1",
        "opset_version": 7,
        "metadata": {
            "model_sha": "ce0987dfa6228ad29708013f38b7f73cbc14ca6066a05fc22ee49b35871c63ed",
            "model_bytes": 574774380,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "vgg0_dense2_fwd",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "flatten_162",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-bn-7.tar.gz",
            "model_with_data_sha": "976427d5cbb8909784fbe3ba997fcf62a67b92c42ffd54f961c0e0748e004b1c",
            "model_with_data_bytes": 532271265
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "2a7d16457887670e978c9bcfc64e686482d1da807fcfbd7536074e47ad4540f7",
            "model_bytes": 574674684,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-3.tar.gz",
            "model_with_data_sha": "8646cc6d0252d3453328da9a15a32e82201bbf6854a865125344e2430c90a4e1",
            "model_with_data_bytes": 536839176
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "088ee597d84957594ded3fd44d669ed142ce2484048e1d9f7fafeb66d12913fa",
            "model_bytes": 574674798,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-6.tar.gz",
            "model_with_data_sha": "289f11496aaa99ca1fe68ea0912b96bfaae4dcc55263187dfdc844e6f5022522",
            "model_with_data_bytes": 536839354
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "967f4f585137782d541301ba2d4a9c006be92671de53e37739d92e386e3124ea",
            "model_bytes": 574674712,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_3",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-7.tar.gz",
            "model_with_data_sha": "4af84e5006a0196facad8add53094424a22809ef97e434215a923b20fc1a528b",
            "model_with_data_bytes": 534082025
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "639ce28961555254d0726c6bb7b2ae7bb9665233d00e04d1add3425aae86cb87",
            "model_bytes": 574674712,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_3",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-8.tar.gz",
            "model_with_data_sha": "1f6254d85d8a102e36a1c2c4ebd7b1f5831d95fd8255a81a0411f24b3c38381e",
            "model_with_data_bytes": 534082067
        }
    },
    {
        "model": "VGG 19-caffe2",
        "model_path": "vision/classification/vgg/model/vgg19-caffe2-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "cfb97260b303ad3963999662456571b89c5f73dbdd2fd9dffd19fed7a90f766b",
            "model_bytes": 574674712,
            "tags": [
                "vision",
                "classification",
                "vgg"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "prob_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "fc7_3",
                        "shape": [
                            1,
                            4096
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/vgg/model/vgg19-caffe2-9.tar.gz",
            "model_with_data_sha": "bf7f96d04f4b635a8a37efd4540ea35958d52201971e91cb74bb55c89e6f4d5c",
            "model_with_data_bytes": 534081932
        }
    },
    {
        "model": "ZFNet-512-int8",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "df166c66ff5626538400506f7871cad4f721bd07bbb2ba2af6d2309c7250917c",
            "model_bytes": 87274426,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-12-int8.tar.gz",
            "model_with_data_sha": "786528fcfddff5da00cb87ac18fda53e84a1f3eaeaa80d3f57a1c8522b6fd3da",
            "model_with_data_bytes": 50270897
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "c975ca6a24c0a2e1584c6f82667e0ac9c06084fc7769d267f71b4a154d1f1c72",
            "model_bytes": 349005501,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/fc7_2",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-12.tar.gz",
            "model_with_data_sha": "8b06a4b6bf8d58f1810828cde5861cc5e8bfd72738be55da41460f965521dfe2",
            "model_with_data_bytes": 323924791
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-3.onnx",
        "onnx_version": "1.1",
        "opset_version": 3,
        "metadata": {
            "model_sha": "7a30178001394e21ccaac288942f4130f0099d4d3263b8ba2272f0cf2a988f49",
            "model_bytes": 349005312,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-3.tar.gz",
            "model_with_data_sha": "de9a1f2db305eb6362d6262758203cd6ba6d29431fef5fb569acde1d3dff1732",
            "model_with_data_bytes": 325256523
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-6.onnx",
        "onnx_version": "1.1.2",
        "opset_version": 6,
        "metadata": {
            "model_sha": "80158bfe8c1191263d003bfaa62694e7cf389b4a472456e389ceadbc4dab684d",
            "model_bytes": 349005426,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-6.tar.gz",
            "model_with_data_sha": "01b7de071b9a17b3654039a648c1571db272ed83af2d58776c0951271e3481da",
            "model_with_data_bytes": 326891392
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "b62d3e67f480d3faa3757dba77f519008bc985072ae163ee0bbcac426916efee",
            "model_bytes": 349005372,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/fc7_2",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-7.tar.gz",
            "model_with_data_sha": "531c9010f14056a332b7148914473dc8bf4f0c42b3d68c9bd23f224d0967ce17",
            "model_with_data_bytes": 324181951
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "2b70b4d43c6e1b8a19b93bb465cb40c2f4f656abac13aebd1b3105e9c904b486",
            "model_bytes": 349005372,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/fc7_2",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-8.tar.gz",
            "model_with_data_sha": "797a462613065ee77f84d549d80d1bfde88314f741a10e8c83d4f406b1bc517b",
            "model_with_data_bytes": 324182001
        }
    },
    {
        "model": "ZFNet-512",
        "model_path": "vision/classification/zfnet-512/model/zfnet512-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "55e3cb74e3d1744e0a8d9fecadd79e9f52887891e2e17df639670ac7293d99ba",
            "model_bytes": 349005372,
            "tags": [
                "vision",
                "classification",
                "zfnet-512"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "gpu_0/data_0",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "gpu_0/softmax_1",
                        "shape": [
                            1,
                            1000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "extra_ports": {
                "features": [
                    {
                        "name": "gpu_0/fc7_2",
                        "shape": [
                            1,
                            1024
                        ]
                    }
                ]
            },
            "model_with_data_path": "vision/classification/zfnet-512/model/zfnet512-9.tar.gz",
            "model_with_data_sha": "4e90c1f5f82fc6e4a1056b42cf967f9a54363bfd3d7f2b404ea260cefbc57518",
            "model_with_data_bytes": 324181833
        }
    },
    {
        "model": "ResNet101_DUC_HDC-12-int8",
        "model_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-12-int8.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "08a3c24af5e375d064d7a4bb80b46191adf5f0f302fd4b5bee7f632ebdadbf67",
            "model_bytes": 65552143,
            "tags": [
                "vision",
                "object detection segmentation",
                "duc"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            800,
                            800
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "seg_loss",
                        "shape": [
                            1,
                            19,
                            160000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-12-int8.tar.gz",
            "model_with_data_sha": "0955ab4564ccb54128639eacc0f57aa3d75be5da0f88c9e7de67ca0c1d286947",
            "model_with_data_bytes": 71077085
        }
    },
    {
        "model": "ResNet101_DUC_HDC-12",
        "model_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-12.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "11ebd46b66706e8bb45e74f67a616f0f61029014e99bd8ea7e55deb74a61e9b6",
            "model_bytes": 260671144,
            "tags": [
                "vision",
                "object detection segmentation",
                "duc"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            800,
                            800
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "seg_loss",
                        "shape": [
                            1,
                            19,
                            160000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-12.tar.gz",
            "model_with_data_sha": "33753b1118846c5e5eda4564c346f615fe1c42db82da8e4ae77c9509380dc46a",
            "model_with_data_bytes": 259342514
        }
    },
    {
        "model": "ResNet101_DUC_HDC",
        "model_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-7.onnx",
        "onnx_version": "1.2.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "6d3b29046b6df3e993ac7fd7253a1070c8e24a0c9690604e15211f78f1efc4e4",
            "model_bytes": 260681709,
            "tags": [
                "vision",
                "object detection segmentation",
                "duc"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "data",
                        "shape": [
                            1,
                            3,
                            800,
                            800
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "seg_loss",
                        "shape": [
                            1,
                            19,
                            160000
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/duc/model/ResNet101-DUC-7.tar.gz",
            "model_with_data_sha": "bd14b6dc0dc49412edbb9828494876cf9afc94ea52496ba6a11794619872b61c",
            "model_with_data_bytes": 259587240
        }
    },
    {
        "model": "Faster R-CNN R-50-FPN",
        "model_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "dfb81423efbea52e45df242ade64cfca0ba05fae78e00cf0c68a0979987f87eb",
            "model_bytes": 167330019,
            "tags": [
                "vision",
                "object detection segmentation",
                "faster-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6379",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6381",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6383",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-10.tar.gz",
            "model_with_data_sha": "99e70d1e9b2902f6d402a235e994ed8242d0b04b59f799eb09ff88bff55acb1d",
            "model_with_data_bytes": 155190684
        }
    },
    {
        "model": "Faster R-CNN R-50-FPN-int8",
        "model_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "95f67f5f6249f4804f1302367dd88cee32bf47713b9858cc6d8ba835548f9b8e",
            "model_bytes": 44631113,
            "tags": [
                "vision",
                "object detection segmentation",
                "faster-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6379",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6381",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6383",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12-int8.tar.gz",
            "model_with_data_sha": "9edf754e3e4e6e78aed9a9d5690299edeba27b17d4dd43635a4fbd4c0e747be0",
            "model_with_data_bytes": 38497565
        }
    },
    {
        "model": "Faster R-CNN R-50-FPN-qdq",
        "model_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12-qdq.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "7b8b530b50b2fa9737625f2bbf454c439e9156c94d44e22602946d019f105f1c",
            "model_bytes": 44525748,
            "tags": [
                "vision",
                "object detection segmentation",
                "faster-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6379",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6381",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6383",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12-qdq.tar.gz",
            "model_with_data_sha": "90673a5a499343863af4c3f4efbed7e35c7a6c587dc37f2d5d2bf0bc77b6865a",
            "model_with_data_bytes": 29844855
        }
    },
    {
        "model": "Faster R-CNN R-50-FPN-fp32",
        "model_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "789104f5a47f008450a37e02da8a8d3642766ed10d91238312b1532fe72789a6",
            "model_bytes": 176713194,
            "tags": [
                "vision",
                "object detection segmentation",
                "faster-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6379",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6381",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6383",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12.tar.gz",
            "model_with_data_sha": "67208685047b27a201c1b50a3a7a0ea25161e9988bc4211adb74c85637c33860",
            "model_with_data_bytes": 163814449
        }
    },
    {
        "model": "FCN ResNet-101",
        "model_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet101-11.onnx",
        "onnx_version": "1.8.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "fd63d026cf4b2e54ef2bffb766914ddbffef9bd7a67eba0e67dbf1514bcf539d",
            "model_bytes": 217069155,
            "tags": [
                "vision",
                "object detection segmentation",
                "fcn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch",
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "out",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "aux",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet101-11.tar.gz",
            "model_with_data_sha": "672ca7f736bff936f6b5fa1c8e07546f5775edc55bf33d62ae21ab372e8f6065",
            "model_with_data_bytes": 294729465
        }
    },
    {
        "model": "FCN ResNet-50",
        "model_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-11.onnx",
        "onnx_version": "1.8.0",
        "opset_version": 11,
        "metadata": {
            "model_sha": "8abd3ae6c258e6fd210d8fb296261ea5c0603c6cc2f568db6097f6fbaaeff0d5",
            "model_bytes": 141193553,
            "tags": [
                "vision",
                "object detection segmentation",
                "fcn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch",
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "out",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "aux",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-11.tar.gz",
            "model_with_data_sha": "05b951717bd910d9194f207373f36eb99d0dedb76c0f2526c83c6a70fb3edf0e",
            "model_with_data_bytes": 223950703
        }
    },
    {
        "model": "FCN ResNet-50-int8",
        "model_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-12-int8.onnx",
        "onnx_version": "1.8.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "67fcba8ebd435c2fb6b297d2e7c084f3c3b9ae6bc7ae41453b748ae4ed8c0331",
            "model_bytes": 35545058,
            "tags": [
                "vision",
                "object detection segmentation",
                "fcn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch",
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "out",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "aux",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-12-int8.tar.gz",
            "model_with_data_sha": "8e81dd5cd5625e8c544f92429e36028596b88d0574ccf72c8ebc48174508cb29",
            "model_with_data_bytes": 30117697
        }
    },
    {
        "model": "FCN ResNet-50",
        "model_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-12.onnx",
        "onnx_version": "1.8.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "eb5017d1b80372eb0b58552655274698817ba2e774437e2c2d3c0a613d2e99bd",
            "model_bytes": 141193555,
            "tags": [
                "vision",
                "object detection segmentation",
                "fcn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch",
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "out",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "aux",
                        "shape": [
                            "batch",
                            21,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/fcn/model/fcn-resnet50-12.tar.gz",
            "model_with_data_sha": "470813ecd5791f20f7b21c54765d8eae51b22bcf2ece668656384476f97d6075",
            "model_with_data_bytes": 131124463
        }
    },
    {
        "model": "Mask R-CNN R-50-FPN",
        "model_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "a519d8102cb162e78cbf123615aa5a8f3bf9d0fa1dec61a2fbbb42fa3f0e0757",
            "model_bytes": 177925424,
            "tags": [
                "vision",
                "object detection segmentation",
                "mask-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6568",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6570",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6572",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6887",
                        "shape": [
                            "nbox",
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-10.tar.gz",
            "model_with_data_sha": "a4cea423312fac6fa9dd0300ace676df4955edc413de66eaaf00d08f30d65738",
            "model_with_data_bytes": 165013602
        }
    },
    {
        "model": "Mask R-CNN R-50-FPN-int8",
        "model_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "4409935e855719fd6cd986f7ec2a3de840d0bd9c9cf7a0cba84ce95377f5b476",
            "model_bytes": 45769352,
            "tags": [
                "vision",
                "object detection segmentation",
                "mask-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6568",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6570",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6572",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6887",
                        "shape": [
                            "nbox",
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12-int8.tar.gz",
            "model_with_data_sha": "0b16a7859d04601a3633a22c07797c9975b877ff384af23d7d9698467d57d333",
            "model_with_data_bytes": 39438079
        }
    },
    {
        "model": "Mask R-CNN R-50-FPN-qdq",
        "model_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12-qdq.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "d6fe3a9f49c3009d745b388c09c06ec3b332d942ba456053411a1f1bff22af9c",
            "model_bytes": 45676737,
            "tags": [
                "vision",
                "object detection segmentation",
                "mask-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6568",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6570",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6572",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6887",
                        "shape": [
                            "nbox",
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12-qdq.tar.gz",
            "model_with_data_sha": "a2b03fc6dcec9bd469ac57dab47ecca97e5b9a28d332435b6820a96925ab78b1",
            "model_with_data_bytes": 30880086
        }
    },
    {
        "model": "Mask R-CNN R-50-FPN-fp32",
        "model_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "5bf940c124d9117f6fae350c4e0d46df180cbb01981832b35fdbed148078b946",
            "model_bytes": 177970010,
            "tags": [
                "vision",
                "object detection segmentation",
                "mask-rcnn"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            3,
                            "height",
                            "width"
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "6568",
                        "shape": [
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6570",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "6572",
                        "shape": [
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "6887",
                        "shape": [
                            "nbox",
                            1,
                            28,
                            28
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/mask-rcnn/model/MaskRCNN-12.tar.gz",
            "model_with_data_sha": "6e41b51713a19119bcf0ef72120ca5362dbdd151112ea91e6a07ae6d3c82265c",
            "model_with_data_bytes": 164901207
        }
    },
    {
        "model": "RetinaNet (ResNet101 backbone)",
        "model_path": "vision/object_detection_segmentation/retinanet/model/retinanet-9.onnx",
        "onnx_version": "1.6.0",
        "opset_version": 9,
        "metadata": {
            "model_sha": "06742923960ec4d9899e6fe407d4d2df013fe6962504f099463ca1b8cba45e44",
            "model_bytes": 228369343,
            "tags": [
                "vision",
                "object detection segmentation",
                "retinanet"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            1,
                            3,
                            480,
                            640
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            720,
                            60,
                            80
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output2",
                        "shape": [
                            1,
                            720,
                            30,
                            40
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output3",
                        "shape": [
                            1,
                            720,
                            15,
                            20
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output4",
                        "shape": [
                            1,
                            720,
                            8,
                            10
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output5",
                        "shape": [
                            1,
                            720,
                            4,
                            5
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output6",
                        "shape": [
                            1,
                            36,
                            60,
                            80
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output7",
                        "shape": [
                            1,
                            36,
                            30,
                            40
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output8",
                        "shape": [
                            1,
                            36,
                            15,
                            20
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output9",
                        "shape": [
                            1,
                            36,
                            8,
                            10
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "output10",
                        "shape": [
                            1,
                            36,
                            4,
                            5
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/retinanet/model/retinanet-9.tar.gz",
            "model_with_data_sha": "3a710eee22a7c2ae8e9d6166774d0fd7b7438146ed2d2e993b9694eb39c3addc",
            "model_with_data_bytes": 153330008
        }
    },
    {
        "model": "SSD-MobilenetV1",
        "model_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_10.onnx",
        "onnx_version": "1.7.0",
        "opset_version": 10,
        "metadata": {
            "model_sha": "1fbcf47654165f2e0b5f1bdf3f123b9e9e1128cd6463717767b76ab4b5246f9a",
            "model_bytes": 29275103,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd-mobilenetv1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image_tensor:0",
                        "shape": [
                            "unk__8520",
                            "unk__8521",
                            "unk__8522",
                            3
                        ],
                        "type": "tensor(uint8)"
                    }
                ],
                "outputs": [
                    {
                        "name": "detection_boxes:0",
                        "shape": [
                            "unk__8523",
                            "unk__8524",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_classes:0",
                        "shape": [
                            "unk__8525",
                            "unk__8526"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_scores:0",
                        "shape": [
                            "unk__8527",
                            "unk__8528"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "num_detections:0",
                        "shape": [
                            "unk__8529"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_10.tar.gz",
            "model_with_data_sha": "7a2ff9ea172ff3a88dbc7890ede008a5ec8e9ce276d866f0a846ed3007f5c3f0",
            "model_with_data_bytes": 25464564
        }
    },
    {
        "model": "SSD-MobilenetV1-12-int8",
        "model_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_12-int8.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "2b79e6a7fb1ec6a33f332b9b10d82d9de4b7b49dcd26b5946921bb356895c954",
            "model_bytes": 9540809,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd-mobilenetv1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "inputs",
                        "shape": [
                            "unk__6578",
                            "unk__6579",
                            "unk__6580",
                            3
                        ],
                        "type": "tensor(uint8)"
                    }
                ],
                "outputs": [
                    {
                        "name": "detection_boxes",
                        "shape": [
                            null,
                            null,
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_classes",
                        "shape": [
                            null,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_scores",
                        "shape": [
                            null,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "num_detections",
                        "shape": [
                            null
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_12-int8.tar.gz",
            "model_with_data_sha": "6c77eb8d85247fceb53bf5da5755023e6c84d8c961fc722cf149f86943f5aea4",
            "model_with_data_bytes": 5804599
        }
    },
    {
        "model": "SSD-MobilenetV1-12",
        "model_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_12.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 12,
        "metadata": {
            "model_sha": "b8fba5e404077d4048d27fcd1667e85e27e192eb9bf51e696c46a3acd7d21058",
            "model_bytes": 29461455,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd-mobilenetv1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "inputs",
                        "shape": [
                            "unk__6578",
                            "unk__6579",
                            "unk__6580",
                            3
                        ],
                        "type": "tensor(uint8)"
                    }
                ],
                "outputs": [
                    {
                        "name": "detection_boxes",
                        "shape": [
                            "unk__6581",
                            "unk__6582",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_classes",
                        "shape": [
                            "unk__6583",
                            "unk__6584"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_scores",
                        "shape": [
                            "unk__6585",
                            "unk__6586"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "num_detections",
                        "shape": [
                            "unk__6587"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_12.tar.gz",
            "model_with_data_sha": "bb7485300ba8379f963e47278d3940cebeedba1fd1370b374aac733414d7cf9b",
            "model_with_data_bytes": 25475584
        }
    },
    {
        "model": "SSD-MobilenetV1-12-qdq",
        "model_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_13-qdq.onnx",
        "onnx_version": "1.9.0",
        "opset_version": 13,
        "metadata": {
            "model_sha": "e5caee2a4bcd7ac7e3d929febaabf1cff9210717abbf56e452c0c496c60ada9a",
            "model_bytes": 10010183,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd-mobilenetv1"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "inputs",
                        "shape": [
                            "unk__6578",
                            "unk__6579",
                            "unk__6580",
                            3
                        ],
                        "type": "tensor(uint8)"
                    }
                ],
                "outputs": [
                    {
                        "name": "detection_boxes",
                        "shape": [
                            null,
                            null,
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_classes",
                        "shape": [
                            null,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "detection_scores",
                        "shape": [
                            null,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "num_detections",
                        "shape": [
                            null
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd-mobilenetv1/model/ssd_mobilenet_v1_13-qdq.tar.gz",
            "model_with_data_sha": "4080066b273a5171b8987a1ac96c6ce6efb935d7c15509089b1934aa63cdd17b",
            "model_with_data_bytes": 5907922
        }
    },
    {
        "model": "SSD",
        "model_path": "vision/object_detection_segmentation/ssd/model/ssd-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "80013dd940efc1462c616143b461ef35885bcc1706e0670b66623293b4b52c59",
            "model_bytes": 80363696,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            1,
                            3,
                            1200,
                            1200
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "bboxes",
                        "shape": [
                            1,
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "labels",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd/model/ssd-10.tar.gz",
            "model_with_data_sha": "5ff54f161d58e02bfc751d3264d58d484485785e71827cf2dd14620bf354c2d0",
            "model_with_data_bytes": 78515627
        }
    },
    {
        "model": "SSD-int8",
        "model_path": "vision/object_detection_segmentation/ssd/model/ssd-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "56d2c03a8c74c03f704509ccfcd91763991c6e1b92f63da730fb2b3d07565453",
            "model_bytes": 20485276,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            1,
                            3,
                            1200,
                            1200
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "bboxes",
                        "shape": [
                            1,
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "labels",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd/model/ssd-12-int8.tar.gz",
            "model_with_data_sha": "723c25c6040e1d5f7fd7ce736e236b09d6881f6f2cc6e7a79a999f3e69f86571",
            "model_with_data_bytes": 31816976
        }
    },
    {
        "model": "SSD-qdq",
        "model_path": "vision/object_detection_segmentation/ssd/model/ssd-12-qdq.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "fba599b77e556525844e4e1022a9f60969b0e9130baa03640539cb0b58f474a2",
            "model_bytes": 20479614,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            1,
                            3,
                            1200,
                            1200
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "bboxes",
                        "shape": [
                            1,
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "labels",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd/model/ssd-12-qdq.tar.gz",
            "model_with_data_sha": "2cd791e3ee5f472a208d292ef766f6b4f04c71463dcf290f9393dcce8b81d6d6",
            "model_with_data_bytes": 26976158
        }
    },
    {
        "model": "SSD",
        "model_path": "vision/object_detection_segmentation/ssd/model/ssd-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "5ec1e058e666ef980a6213ac40088877865aafb2b6676b9af204b3828e4bb9d5",
            "model_bytes": 80366315,
            "tags": [
                "vision",
                "object detection segmentation",
                "ssd"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            1,
                            3,
                            1200,
                            1200
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "bboxes",
                        "shape": [
                            1,
                            "nbox",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "labels",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(int64)"
                    },
                    {
                        "name": "scores",
                        "shape": [
                            1,
                            "nbox"
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/ssd/model/ssd-12.tar.gz",
            "model_with_data_sha": "43e57501da34abe0c01ea30902303bdb14f590f59c18fbb15ea3473810b0e86b",
            "model_with_data_bytes": 90592544
        }
    },
    {
        "model": "Tiny YOLOv2",
        "model_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-7.onnx",
        "onnx_version": "1.2",
        "opset_version": 7,
        "metadata": {
            "model_sha": "87befe217358b6beda0b496536b17216ebddef8f70e8d86fe34ed089bb577289",
            "model_bytes": 63480982,
            "tags": [
                "vision",
                "object detection segmentation",
                "tiny-yolov2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            "None",
                            3,
                            416,
                            416
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "grid",
                        "shape": [
                            "None",
                            125,
                            13,
                            13
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-7.tar.gz",
            "model_with_data_sha": "b20099da6c3d78ee60f1a68073eb2b522dd572c32b1787f588b822afd2d2e34c",
            "model_with_data_bytes": 60865248
        }
    },
    {
        "model": "Tiny YOLOv2",
        "model_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-8.onnx",
        "onnx_version": "1.3",
        "opset_version": 8,
        "metadata": {
            "model_sha": "583fb7fdc948435ceac9fa82efc7708701efe8382a859a3dd46526b155f5f2ae",
            "model_bytes": 63480982,
            "tags": [
                "vision",
                "object detection segmentation",
                "tiny-yolov2"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "image",
                        "shape": [
                            "None",
                            3,
                            416,
                            416
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "grid",
                        "shape": [
                            "None",
                            125,
                            13,
                            13
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/tiny-yolov2/model/tinyyolov2-8.tar.gz",
            "model_with_data_sha": "5122c3e7dd199668fd1d99e1e36132c3ca71ae6c18c3470e7d0e3e7570f8dfbd",
            "model_with_data_bytes": 60864927
        }
    },
    {
        "model": "Tiny YOLOv3",
        "model_path": "vision/object_detection_segmentation/tiny-yolov3/model/tiny-yolov3-11.onnx",
        "onnx_version": "1.6",
        "opset_version": 11,
        "metadata": {
            "model_sha": "f715cc2d99740d22d312777e20d9de2b2ecdc250155be8fd3752ce7e8b823521",
            "model_bytes": 35511756,
            "tags": [
                "vision",
                "object detection segmentation",
                "tiny-yolov3"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1",
                        "shape": [
                            "N",
                            3,
                            null,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "image_shape",
                        "shape": [
                            "N",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "yolonms_layer_1",
                        "shape": [
                            1,
                            null,
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1:1",
                        "shape": [
                            1,
                            80,
                            null
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1:2",
                        "shape": [
                            1,
                            null,
                            3
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/tiny-yolov3/model/tiny-yolov3-11.tar.gz",
            "model_with_data_sha": "acdf6c64b01ad574c419efe25d8babcc17d10d49e73cd1f636b1394e1af914ee",
            "model_with_data_bytes": 34058193
        }
    },
    {
        "model": "YOLOv2",
        "model_path": "vision/object_detection_segmentation/yolov2-coco/model/yolov2-coco-9.onnx",
        "onnx_version": "1.5",
        "opset_version": 9,
        "metadata": {
            "model_sha": "a2c44ecf4860acdf03193d41b7d2957637d0b14b8a9e339463b892b0acb9a12f",
            "model_bytes": 203948401,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov2-coco"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input.1",
                        "shape": [
                            1,
                            3,
                            416,
                            416
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "218",
                        "shape": [
                            1,
                            425,
                            13,
                            13
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov2-coco/model/yolov2-coco-9.tar.gz",
            "model_with_data_sha": "c34966e4e96165f8db6f2549f509b6a8bdfc01ddd978e6fd07daad4e665d5383",
            "model_with_data_bytes": 191439022
        }
    },
    {
        "model": "YOLOv3",
        "model_path": "vision/object_detection_segmentation/yolov3/model/yolov3-10.onnx",
        "onnx_version": "1.5",
        "opset_version": 10,
        "metadata": {
            "model_sha": "1f4613c3d04416dfd2c1960b8737aa5292994238dfecbe9c1ee7147e9a92439f",
            "model_bytes": 247908721,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov3"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1",
                        "shape": [
                            "unk__576",
                            3,
                            "unk__577",
                            "unk__578"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "image_shape",
                        "shape": [
                            "unk__579",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "yolonms_layer_1/ExpandDims_1:0",
                        "shape": [
                            1,
                            "unk__581",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/ExpandDims_3:0",
                        "shape": [
                            1,
                            80,
                            "unk__584"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/concat_2:0",
                        "shape": [
                            "unk__585",
                            "unk__586"
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov3/model/yolov3-10.tar.gz",
            "model_with_data_sha": "13ca9e9fba6f2f1bcf31d62cf9b9586a04c49b5f2b8c23bc288b9b6706126c97",
            "model_with_data_bytes": 232571698
        }
    },
    {
        "model": "YOLOv3-12-int8",
        "model_path": "vision/object_detection_segmentation/yolov3/model/yolov3-12-int8.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "9230c02fb8e578c6ecfeeb3f0d4fee1bf05bf9c11d4e0399550d6d57a9ad7b5c",
            "model_bytes": 62912863,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov3"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1",
                        "shape": [
                            "unk__576",
                            3,
                            "unk__577",
                            "unk__578"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "image_shape",
                        "shape": [
                            "unk__579",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "yolonms_layer_1/ExpandDims_1:0",
                        "shape": [
                            1,
                            "unk__581",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/ExpandDims_3:0",
                        "shape": [
                            1,
                            80,
                            "unk__584"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/concat_2:0",
                        "shape": [
                            "unk__585",
                            3
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov3/model/yolov3-12-int8.tar.gz",
            "model_with_data_sha": "1cbd40e1489553cd5fb03a0eb3059af9f8f0262a963f023dce0ecf3b9dcf90c6",
            "model_with_data_bytes": 58205845
        }
    },
    {
        "model": "YOLOv3-12",
        "model_path": "vision/object_detection_segmentation/yolov3/model/yolov3-12.onnx",
        "onnx_version": "1.9",
        "opset_version": 12,
        "metadata": {
            "model_sha": "d97d7d5c1bbab2ec1ea197f0b689eb121231d62985ad223fb8aa833f8ef79836",
            "model_bytes": 247942267,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov3"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1",
                        "shape": [
                            "unk__576",
                            3,
                            "unk__577",
                            "unk__578"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "image_shape",
                        "shape": [
                            "unk__579",
                            2
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "yolonms_layer_1/ExpandDims_1:0",
                        "shape": [
                            1,
                            "unk__581",
                            4
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/ExpandDims_3:0",
                        "shape": [
                            1,
                            80,
                            "unk__584"
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "yolonms_layer_1/concat_2:0",
                        "shape": [
                            "unk__585",
                            3
                        ],
                        "type": "tensor(int32)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov3/model/yolov3-12.tar.gz",
            "model_with_data_sha": "bcd5021e0aaa492ce9f629466974092244710337d07e6887fc7854932b6ae0a3",
            "model_with_data_bytes": 232651839
        }
    },
    {
        "model": "YOLOv4",
        "model_path": "vision/object_detection_segmentation/yolov4/model/yolov4.onnx",
        "onnx_version": "1.6",
        "opset_version": 11,
        "metadata": {
            "model_sha": "1881fe9c506c970d7866cb4bfc33bda791ce46951a3c39c45ace2ff2b9daf369",
            "model_bytes": 257470589,
            "tags": [
                "vision",
                "object detection segmentation",
                "yolov4"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input_1:0",
                        "shape": [
                            "unk__2104",
                            416,
                            416,
                            3
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "Identity:0",
                        "shape": [
                            "unk__2105",
                            "unk__2106",
                            "unk__2107",
                            3,
                            85
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "Identity_1:0",
                        "shape": [
                            "unk__2108",
                            "unk__2109",
                            "unk__2110",
                            3,
                            85
                        ],
                        "type": "tensor(float)"
                    },
                    {
                        "name": "Identity_2:0",
                        "shape": [
                            "unk__2111",
                            "unk__2112",
                            "unk__2113",
                            3,
                            85
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/object_detection_segmentation/yolov4/model/yolov4.tar.gz",
            "model_with_data_sha": "7d06db57b6b4fff4f75dd3bd16f29b32faec4552d763869d5a3c37d25542cbb2",
            "model_with_data_bytes": 243453644
        }
    },
    {
        "model": "Candy",
        "model_path": "vision/style_transfer/fast_neural_style/model/candy-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "e3c6f442e12014c5fec12b4307c14ca303df185cda19f67522afeef40c4d22b8",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/candy-8.tar.gz",
            "model_with_data_sha": "e6a40c8d1b5f6b680f4c770ac9ec5a26638fea1439bb2561e71a3a4a55c1e8a7",
            "model_with_data_bytes": 7338783
        }
    },
    {
        "model": "Candy",
        "model_path": "vision/style_transfer/fast_neural_style/model/candy-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "9d11a3529d1e547da6ae07201d93484dbab2ec0a3614535752c8f40f0fe2968a",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/candy-9.tar.gz",
            "model_with_data_sha": "7647c08408079b400f95526bf15e21a247af2112ca7019396fc6945b76f9cb33",
            "model_with_data_bytes": 7338825
        }
    },
    {
        "model": "Mosaic",
        "model_path": "vision/style_transfer/fast_neural_style/model/mosaic-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "8547ad01962f2709c2ca6732345c74d14cb1dd79c09c92365ac113cdb6c2eac5",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/mosaic-8.tar.gz",
            "model_with_data_sha": "2b3090c84a0e28fe00fc162b764c8946186a33535eee6250e2595429ac309346",
            "model_with_data_bytes": 7333067
        }
    },
    {
        "model": "Mosaic",
        "model_path": "vision/style_transfer/fast_neural_style/model/mosaic-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "fa646dedade881243f8d5a2ceb7de2b93675b21fc24f7482894ac4851a9a0a47",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/mosaic-9.tar.gz",
            "model_with_data_sha": "e7d395a1a2d8776a2bdf27adf9b581a5bb2dc94d2ea47c54791cdc55afcae7a0",
            "model_with_data_bytes": 7333449
        }
    },
    {
        "model": "Pointilism",
        "model_path": "vision/style_transfer/fast_neural_style/model/pointilism-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "09fb31931c1826daf42552e451a908d068486a3ca946324a3ece758d51f46cad",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/pointilism-8.tar.gz",
            "model_with_data_sha": "e67096dba8f00a30b86422868563f1c299565592314778e42033b87d46ba1ca8",
            "model_with_data_bytes": 7333381
        }
    },
    {
        "model": "Pointilism",
        "model_path": "vision/style_transfer/fast_neural_style/model/pointilism-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "5ee2b8d4d6bc60a777f54e0fe96a1b717360a004b79d56c67390d4a975b14d98",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/pointilism-9.tar.gz",
            "model_with_data_sha": "ceac242d7e058391b317fccc656c43be9e58a148036027dd38d6b1bd4ac040fd",
            "model_with_data_bytes": 7335554
        }
    },
    {
        "model": "Rain Princess",
        "model_path": "vision/style_transfer/fast_neural_style/model/rain-princess-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "bf13b623576021a77dab6aa54c366d0900e8f55909be27cf4a40248d6ff6e097",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/rain-princess-8.tar.gz",
            "model_with_data_sha": "483c273fd3c293417250329cb9c64d68c19c1cf5a1c8e4a375ff92a00b3cb972",
            "model_with_data_bytes": 7344452
        }
    },
    {
        "model": "Rain Princess",
        "model_path": "vision/style_transfer/fast_neural_style/model/rain-princess-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "4162912e6f75fedef6f810ae989b9e10d3d5d43308dab34b027c850cf255e152",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/rain-princess-9.tar.gz",
            "model_with_data_sha": "8c35f1d7c72c3d89ff2b61163a9715b3701c80603a38b6df28373162d48e895a",
            "model_with_data_bytes": 7344974
        }
    },
    {
        "model": "Udnie",
        "model_path": "vision/style_transfer/fast_neural_style/model/udnie-8.onnx",
        "onnx_version": "1.4",
        "opset_version": 8,
        "metadata": {
            "model_sha": "31f8c5457a0d850b55564b018f6e494e49b3392c50eee6662799a47491b4a933",
            "model_bytes": 6726529,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/udnie-8.tar.gz",
            "model_with_data_sha": "81768986c46484c6f48f6c43d8ba267a9fa52e5eed691239e60c26e3f672299c",
            "model_with_data_bytes": 7339429
        }
    },
    {
        "model": "Udnie",
        "model_path": "vision/style_transfer/fast_neural_style/model/udnie-9.onnx",
        "onnx_version": "1.4",
        "opset_version": 9,
        "metadata": {
            "model_sha": "8656b6ce7dec8f22ee13c2d557d6b67bd6f550dde88d0f2e7c9972aeb765cc0d",
            "model_bytes": 6728029,
            "tags": [
                "vision",
                "style transfer",
                "fast neural style"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output1",
                        "shape": [
                            1,
                            3,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/style_transfer/fast_neural_style/model/udnie-9.tar.gz",
            "model_with_data_sha": "713f43d88764577218b25e4820b4fc75fe1b547d59c9bfcfa0349cbdbfd84a1e",
            "model_with_data_bytes": 7338543
        }
    },
    {
        "model": "Super_Resolution",
        "model_path": "vision/super_resolution/sub_pixel_cnn_2016/model/super-resolution-10.onnx",
        "onnx_version": "1.5.0",
        "opset_version": 10,
        "metadata": {
            "model_sha": "85f36ff88cc504a24af5e0602148bc56a8aa09a58eca8c0da2756f3e8186035e",
            "model_bytes": 240078,
            "tags": [
                "vision",
                "super resolution",
                "sub pixel cnn 2016"
            ],
            "io_ports": {
                "inputs": [
                    {
                        "name": "input",
                        "shape": [
                            "batch_size",
                            1,
                            224,
                            224
                        ],
                        "type": "tensor(float)"
                    }
                ],
                "outputs": [
                    {
                        "name": "output",
                        "shape": [
                            "batch_size",
                            1,
                            672,
                            672
                        ],
                        "type": "tensor(float)"
                    }
                ]
            },
            "model_with_data_path": "vision/super_resolution/sub_pixel_cnn_2016/model/super-resolution-10.tar.gz",
            "model_with_data_sha": "15810206d8a5a57a6cc4caa9aa2bd712d3a144a2098b2ec31b5074714c4aeeac",
            "model_with_data_bytes": 2079037
        }
    }
]